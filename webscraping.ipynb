{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> webscraping\n",
    "\n",
    "## Première partie du projet bloc 1\n",
    "\n",
    "- Webscraping du site [www.allocine.fr](https://www.allocine.fr/films/) avec Beautiful Soup et Selenium.\n",
    "\n",
    "Nous vérifions tout d'abord le fichier \"robots.txt\" pour voir si nous sommes autorisés à scraper le film.\n",
    "\n",
    "![robots.txt](images/robots.txt.png)\n",
    "\n",
    "Il n'y a pas de limitation pour notre tâche puisque nous travaillons sous l'url : <code>https://www.allocine.fr/film/</code><br>\n",
    "\n",
    "Ensuite nous allons sur le site allocine, catégorie **films** et nous allons scraper les informations à partir des menus déroulants sur la gauche (les catégories de films, les pays et ensuite nous scraperons les films par année).\n",
    "\n",
    "![filtres](images/filtresSMALL.png)\n",
    "\n",
    "## Sources :\n",
    "**Beautiful Soup** :\n",
    "[beautiful-soup-4](https://beautiful-soup-4.readthedocs.io/en/latest/)<br>\n",
    "[beautiful-soup-4.readthedocs.io](https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree)<br>\n",
    "\n",
    "**Selenium** :<br>\n",
    "[selenium-python.readthedocs.io](https://selenium-python.readthedocs.io/locating-elements.html)<br>\n",
    "[selenium.dev/documentation](https://www.selenium.dev/documentation/webdriver/elements/information/)<br>\n",
    "[selenium.dev/documentation/finders/](https://www.selenium.dev/documentation/webdriver/elements/finders/)<br>\n",
    "[geeksforgeeks.org/get_property-selenium/](https://www.geeksforgeeks.org/get_property-element-method-selenium-python/)<br>\n",
    "\n",
    "Les liens sont sûrement générés aléatoirement dynamiquement, on peut utiliser XPath avec selenium<br>\n",
    "ou bien avec lxml ??<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import re\n",
    "import io\n",
    "import math\n",
    "import copy\n",
    "import httpx\n",
    "import requests\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "url_site = 'https://www.allocine.fr/'\n",
    "url_films = 'https://www.allocine.fr/films/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On scrape la liste des genres de film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "Nb categories : 37\n"
     ]
    }
   ],
   "source": [
    "# Scrap all categories\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(type(soup))\n",
    "\n",
    "categories = []\n",
    "elt_categories = soup.find('div', class_='filter-entity-section')\n",
    "for elt in elt_categories.find_all('li'):\n",
    "    #print(elt.prettify())\n",
    "    categories.append(elt.a.text)\n",
    "\n",
    "print(\"Nb categories :\", len(categories))\n",
    "df_categories = pd.Series(categories)\n",
    "\n",
    "dict_n_cat = {k:v for k, v in enumerate(categories)}\n",
    "dict_cat_n = {v:k for v, k in dict_n_cat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On scrape la liste des pays d'origine des films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'U.S.A.', 'Afrique du Sud', 'Albanie', 'Algérie', 'Allemagne', \"Allemagne de l'Est\", \"Allemagne de l'Ouest\", 'Arabie Saoudite', 'Argentine', 'Arménie', 'Australie', 'Autriche', 'Belgique', 'Bengladesh', 'Bolivie', 'Bosnie-Herzégovine', 'Brésil', 'Bulgarie', 'Burkina Faso', 'Cambodge', 'Cameroun', 'Canada', 'Chili', 'Chine', 'Chypre', 'Colombie', 'Corée', 'Corée du Sud', 'Croatie', 'Cuba', \"Côte-d'Ivoire\", 'Danemark', 'Egypte', 'Emirats Arabes Unis', 'Espagne', 'Estonie', 'Finlande', 'Grande-Bretagne', 'Grèce', 'Géorgie', 'Hong-Kong', 'Hongrie', 'Inde', 'Indonésie', 'Irak', 'Iran', 'Irlande', 'Islande', 'Israël', 'Italie', 'Japon', 'Jordanie', 'kazakhstan', 'Kenya', 'Kosovo', 'Lettonie', 'Liban', 'Lituanie', 'Luxembourg', 'Macédoine', 'Malaisie', 'Maroc', 'Mexique', 'Monténégro', 'Nigéria', 'Norvège', 'Nouvelle-Zélande', 'Pakistan', 'Palestine', 'Pays-Bas', 'Philippines', 'Pologne', 'Portugal', 'Pérou', 'Qatar', 'Roumanie', 'Russie', 'République dominicaine', 'République tchèque', 'Serbie', 'Singapour', 'Slovaquie', 'Slovénie', 'Sri Lanka', 'Suisse', 'Suède', 'Syrie', 'Sénégal', 'Taïwan', 'Tchécoslovaquie', 'Thaïlande', 'Tunisie', 'Turquie', 'Ukraine', 'URSS', 'Uruguay', 'Vietnam', 'Vénézuela', 'Yougoslavie']\n",
      "Nb pays : 100\n"
     ]
    }
   ],
   "source": [
    "# Scrap all countries\n",
    "elt_countries = elt_categories.find_next_sibling().find_next_sibling()\n",
    "elts_items = elt_countries.find_all('li', class_ = 'filter-entity-item')\n",
    "\n",
    "countries = []\n",
    "for elt_item in elts_items:\n",
    "    countries.append(elt_item.find('span').text.strip())\n",
    "\n",
    "print(countries)\n",
    "\n",
    "print(\"Nb pays :\", len(countries))\n",
    "df_countries = pd.Series(countries)\n",
    "\n",
    "dict_n_countries = {k:v for k, v in enumerate(countries)}\n",
    "dict_countries_n = {v:k for v, k in dict_n_countries.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On scrape les liens redirigeants vers les pages de films par année\n",
    "\n",
    "**Nous utilisons Selenium**\n",
    "Certains liens sont **décorés** et ne peuvent pas être directement scrapés avec **Beautiful Soup** nous utiliser Selenium qui permet entre autre :\n",
    "- d'utiliser les XPath (contrairement à Beautiful Soup)\n",
    "- de récupérer certains élements **décorés** par exemple des urls\n",
    "\n",
    "On se donne une liste d'années, par exemple [1980, ... 2000]\n",
    "\n",
    "[1980 - 1989] puis [1990 - 1999] ... jusqu'à [2000 - 2009].<br>\n",
    "(cela fait plus de 40000 films et XXX reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1980   ----  link https://www.allocine.fr/films/decennie-1980/annee-1980/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scrap the links of the years\n",
    "\n",
    "# Input: list of years to scrap\n",
    "lst_years_to_scrap = list(range(1980, 2009))\n",
    "lst_years_to_scrap = [1980]\n",
    "\n",
    "lst_decades_to_scrap = list(set([10 * (year // 10) for year in lst_years_to_scrap]))\n",
    "lst_years_to_scrap = [str(year) for year in lst_years_to_scrap]\n",
    "lst_decades_to_scrap = [str(decade) for decade in lst_decades_to_scrap]\n",
    "\n",
    "driver = webdriver.Chrome(options = options)\n",
    "driver.get(url_films)\n",
    "elts_decades = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section[3]/div[1]/div/div[3]/div[2]/ul/li')\n",
    "\n",
    "dict_year_link = {}\n",
    "for elt_decade in tqdm(elts_decades):\n",
    "    elt_a = elt_decade.find_element(By.TAG_NAME, 'a')\n",
    "    if not(elt_a.get_attribute('title')[:4] in lst_decades_to_scrap):\n",
    "        continue\n",
    "\n",
    "    driver2 = webdriver.Chrome(options = options)\n",
    "    url_decade = elt_a.get_attribute('href').strip()\n",
    "    driver2.get(url_decade)\n",
    "    elts_years = driver2.find_elements(By.XPATH, '/html/body/div[2]/main/section[3]/div[1]/div/div[3]/div[3]/ul/li')\n",
    "\n",
    "    for elt_year in elts_years:\n",
    "        year = elt_year.find_element(By.TAG_NAME, 'a').get_attribute('title').strip()\n",
    "        if year in lst_years_to_scrap:\n",
    "            link = elt_year.find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n",
    "            dict_year_link[year] = link\n",
    "    driver2.close()\n",
    "\n",
    "for year, url_year in dict_year_link.items():\n",
    "    print(\"year\", year, '  ----  link', url_year)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On scrape les films à partir des liens vers les années\n",
    "\n",
    "Pour scraper les directeurs et acteurs nous allons sur la page **casting** du film puis nous scrapons les acteurs principaux représentés dans la mosaïque, ensuite nous scrapons la liste des acteurs secondaires.\n",
    "\n",
    "![all_actors](images/scraping_all_actors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping year: 1980\n",
      "---------------------------------------------------------------- \n",
      "Title: Elephant Man\n",
      "Date: 8 avril 1981\n",
      "Duration: 2h 05min\n",
      "Categories: Biopic,Drame\n",
      "Directors: ['David Lynch']\n",
      "Actors: ['photo de Anthony Hopkins', 'photo de John Hurt', 'photo de Anne Bancroft', 'photo de John Gielgud', 'photo de Wendy Hiller', 'photo de Freddie Jones', 'photo de Hannah Gordon', 'photo de Michael Elphick', 'Lesley Dunlop', 'Kenny Baker', 'John Standing', 'Dexter Fletcher', 'Phoebe Nicholls', 'Patsy Smart', 'Frederick Treves', 'Roy Evans', 'Tony London']\n",
      "Composers: ['John Morris (II)']\n",
      "Summary: Londres, 1884. Le chirurgien Frederick Treves découvre un homme complètement défiguré et difforme, devenu une attraction de foire. John Merrick, \" le monstre \", doit son nom de Ele\n",
      "Thumbnail: https://fr.web.img4.acsta.net/c_310_420/pictures/20/02/21/16/48/4302324.jpg\n",
      "3\n",
      "\n",
      "                        <a class=\"xXx rating-title\" href=\"/film/fichefilm-180/critiques/presse/\"> Presse </a>\n",
      "            \n",
      "            \n",
      "                                        \n",
      "            \n",
      "\n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "  \n",
      "<div class=\"stareval stareval-small stareval-theme-default\"><div class=\"rating-mdl n50 stareval-stars\"><div class=\"star icon\"></div><div class=\"star icon\"></div><div class=\"star icon\"></div><div class=\"star icon\"></div><div class=\"star icon\"></div></div><span class=\"stareval-note\">4,8</span><span class=\"stareval-review light\"> 5 critiques</span></div>\n",
      "        \n",
      "*************************************\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".xXx rating-title\"}\n  (Session info: chrome=132.0.6834.83); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF6650FFBE5+28741]\n\t(No symbol) [0x00007FF665067890]\n\t(No symbol) [0x00007FF664F04FDA]\n\t(No symbol) [0x00007FF664F59322]\n\t(No symbol) [0x00007FF664F5954C]\n\t(No symbol) [0x00007FF664FA3337]\n\t(No symbol) [0x00007FF664F7FF4F]\n\t(No symbol) [0x00007FF664FA00B4]\n\t(No symbol) [0x00007FF664F7FCB3]\n\t(No symbol) [0x00007FF664F49FB3]\n\t(No symbol) [0x00007FF664F4B331]\n\tGetHandleVerifier [0x00007FF66543A73D+3414941]\n\tGetHandleVerifier [0x00007FF66544E64A+3496618]\n\tGetHandleVerifier [0x00007FF66544413D+3454365]\n\tGetHandleVerifier [0x00007FF6651C848B+850155]\n\t(No symbol) [0x00007FF6650737FF]\n\t(No symbol) [0x00007FF66506F0C4]\n\t(No symbol) [0x00007FF66506F25D]\n\t(No symbol) [0x00007FF66505E079]\n\tBaseThreadInitThunk [0x00007FFF55F17374+20]\n\tRtlUserThreadStart [0x00007FFF57A9CC91+33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 261\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m elt_movie \u001b[38;5;129;01min\u001b[39;00m elt_movies[:\u001b[38;5;241m15\u001b[39m]:\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------------------------------------------------------- \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 261\u001b[0m         \u001b[43mscrap_movie\u001b[49m\u001b[43m(\u001b[49m\u001b[43melt_movie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 217\u001b[0m, in \u001b[0;36mscrap_movie\u001b[1;34m(elt_movie, driver)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThumbnail:\u001b[39m\u001b[38;5;124m\"\u001b[39m, url_thumbnail)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# save_thumbnail(title, url_thumbnail)\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# ------------ #\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m#    ratings   #\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# ------------ #\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m \u001b[43mget_ratings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup_movie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 114\u001b[0m, in \u001b[0;36mget_ratings\u001b[1;34m(soup_movie, driver)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(elt_rating_content\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minnerHTML\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*************************************\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 114\u001b[0m elt \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxXx rating-title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(elt\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minnerHTML\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\envs\\block1\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:770\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\envs\\block1\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:384\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Utilisateur\\anaconda3\\envs\\block1\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".xXx rating-title\"}\n  (Session info: chrome=132.0.6834.83); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF6650FFBE5+28741]\n\t(No symbol) [0x00007FF665067890]\n\t(No symbol) [0x00007FF664F04FDA]\n\t(No symbol) [0x00007FF664F59322]\n\t(No symbol) [0x00007FF664F5954C]\n\t(No symbol) [0x00007FF664FA3337]\n\t(No symbol) [0x00007FF664F7FF4F]\n\t(No symbol) [0x00007FF664FA00B4]\n\t(No symbol) [0x00007FF664F7FCB3]\n\t(No symbol) [0x00007FF664F49FB3]\n\t(No symbol) [0x00007FF664F4B331]\n\tGetHandleVerifier [0x00007FF66543A73D+3414941]\n\tGetHandleVerifier [0x00007FF66544E64A+3496618]\n\tGetHandleVerifier [0x00007FF66544413D+3454365]\n\tGetHandleVerifier [0x00007FF6651C848B+850155]\n\t(No symbol) [0x00007FF6650737FF]\n\t(No symbol) [0x00007FF66506F0C4]\n\t(No symbol) [0x00007FF66506F25D]\n\t(No symbol) [0x00007FF66505E079]\n\tBaseThreadInitThunk [0x00007FFF55F17374+20]\n\tRtlUserThreadStart [0x00007FFF57A9CC91+33]\n"
     ]
    }
   ],
   "source": [
    "def number_pages_per_year(soup_year):\n",
    "    ''' Return the number of pages for one year'''\n",
    "    pagination = soup_year.find('div', class_='pagination-item-holder')\n",
    "    nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "    return int(nb_pages)\n",
    "\n",
    "def delete_thumbnails():\n",
    "    '''Delete all files in thumbnail directory'''\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        files = os.listdir(folder_name)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_name, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        print(\"All files deleted successfully.\")\n",
    "    except OSError:\n",
    "        print(\"Error occurred while deleting files.\")\n",
    "\n",
    "def get_title(soup_movie):\n",
    "    ''' Return the title of the movie '''\n",
    "    return soup_movie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text\n",
    "\n",
    "def get_date_duration_categories(soup_movie):\n",
    "    ''' Return date, duration and categories (as string) of the movie '''\n",
    "    elt = soup_movie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "    s1, s2, s3 = text.split('|')\n",
    "    date = s1[:-8].strip()\n",
    "    duration = s2.strip()\n",
    "    categories = s3.strip()\n",
    "    return date, duration, categories\n",
    "\n",
    "def get_directors(soup_casting):\n",
    "    ''' Return list of directors '''\n",
    "    elt_director_section = soup_casting.find('section', class_='section casting-director')\n",
    "    elt_temp = elt_director_section.find_next()\n",
    "    elts_directors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_directors = [elt_director.find('a').text for elt_director in elts_directors]\n",
    "    return lst_directors\n",
    "\n",
    "def get_actors(soup_casting):\n",
    "    ''' Return list of actors (maximum 30) '''\n",
    "    elt_actor_section = soup_casting.find('section', class_ = 'section casting-actor')\n",
    "    if not(elt_actor_section):\n",
    "        return []\n",
    "    elt_temp = elt_actor_section.find_next()\n",
    "    # scrap main actors (maximum eight actors in the mosaic, see image above)\n",
    "    elts_actors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_actors = [elt_actor.find('figure').find('span')['title'] for elt_actor in elts_actors]\n",
    "    elts_actors = elt_actor_section.find_all('div', class_ = 'md-table-row')\n",
    "    # scrap list of actors below the mosaic (we scrap maximum of (8 + 22) 30 actors in total)\n",
    "    lst_actors.extend([elt_actor.find('a').text for elt_actor in elts_actors[:22] if elt_actor.find('a')])\n",
    "    return lst_actors\n",
    "\n",
    "def get_composers(soup_casting):\n",
    "    ''' Scrap the name(s) of the music composer(s) '''\n",
    "    elts_sections = soup_casting.find_all(\"div\", class_ = \"section casting-list-gql\")\n",
    "    for elt_section in elts_sections:\n",
    "        elt_title = elt_section.find('div', class_ = 'titlebar section-title').find('h2')\n",
    "        if 'Soundtrack' in elt_title.text:\n",
    "            lst_composers = []\n",
    "            elts_composers = elt_section.find_all('div', class_ = 'md-table-row')\n",
    "            for elt_composer in elts_composers:\n",
    "                elts_span = elt_composer.find_all('span')\n",
    "                if len(elts_span) > 1 and 'Compositeur' in elts_span[1].text.strip():\n",
    "                    lst_composers.append(elts_span[0].text.strip())\n",
    "            return lst_composers\n",
    "\n",
    "def get_summary(soup_movie):\n",
    "    elt = soup_movie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    return elt.find('p', class_ = 'bo-p').text.strip()\n",
    "\n",
    "def get_thumbnail(soup_movie):\n",
    "    elt = soup_movie.find('figure', class_ = 'thumbnail')\n",
    "    return elt.span.img['src']\n",
    "\n",
    "def save_thumbnail(title, url_thumbnail):\n",
    "    '''Save the thumbnail as image file in directory \"thumbnails\"'''\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        title2 = title.replace('-', '')\n",
    "        image_name = f\"thumbnail-{title2}.jpg\"\n",
    "        file = open(folder_name + image_name, \"wb\")\n",
    "        image = httpx.get(url_thumbnail)\n",
    "        file.write(image.content)\n",
    "        # Display thumbnail in Jupyter / console\n",
    "        img = Image.open(io.BytesIO(image.content))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        # To change resolution: https://www.geeksforgeeks.org/change-image-resolution-using-pillow-in-python/\n",
    "    except IOError:\n",
    "        print(\"Cannot read the file\")\n",
    "    finally:\n",
    "        file.close()\n",
    "\n",
    "def get_ratings(soup_movie, driver):\n",
    "    ''' Scrap the ratings of the movie:\n",
    "\n",
    "        - star rating (0.5 to 5 with step 0.5),\n",
    "        - number of votes,\n",
    "        - number of critics (written reviews).\n",
    "    '''\n",
    "\n",
    "    # print(test.get_attribute('innerHTML'))\n",
    "    # print(driver.current_url)\n",
    "    elts_rating_content = driver.find_elements(By.CLASS_NAME, 'rating-item-content')\n",
    "    print(len(elts_rating_content))\n",
    "\n",
    "    for elt_rating_content in elts_rating_content:\n",
    "        print(elt_rating_content.get_attribute('innerHTML'))\n",
    "        print('*************************************')\n",
    "        elt = driver.find_element(By.CLASS_NAME, 'xXx rating-title')\n",
    "        if elt != None:\n",
    "            print(elt.get_attribute('innerHTML'))\n",
    "        # if 'Spectateur' in elt_rating_content.find_element(By.CLASS_NAME, 'xXx rating-title').text:\n",
    "        #     print(elt_rating_content.find_element(By.CLASS_NAME, 'xXx rating-title').get_attribute('href'))\n",
    "            # return\n",
    "\n",
    "    return\n",
    "    elts_rating = soup_movie.find_all('div', class_='rating-item')\n",
    "    print(\"nb elt_ratings:\", len(elts_rating))\n",
    "    for elt_rating in elts_rating:\n",
    "        elt_content = elt_rating.find('div', class_ = 'rating-item-content')\n",
    "        if 'Spectateurs' in elt_content.find('span').text:\n",
    "            # print(elt_content.prettify())\n",
    "            elt_star_rating = elt_content.find('span', class_ = 'stareval-note')\n",
    "            star_rating = elt_star_rating.text.strip()\n",
    "            elt_nb_notes_reviews = elt_star_rating.find_next_sibling()\n",
    "            print(star_rating, elt_nb_notes_reviews.text.strip())\n",
    "\n",
    "def similar_movies(soup_movies):\n",
    "    pass    \n",
    "\n",
    "def scrap_movie(elt_movie, driver):\n",
    "    ''' scrap all movie informations '''\n",
    "    \n",
    "    # get the movie soup\n",
    "    # print(url_site, elt_movie.h2.a.get('href'))\n",
    "    url_movie = url_site + elt_movie.h2.a.get('href')[1:]\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soup_movie = BeautifulSoup(r.content, 'html.parser')\n",
    "    # print(soup_movie.prettify())\n",
    "    # print(type(driver), url_movie)\n",
    "    driver.get(url_movie)\n",
    "    # elts = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section/div/div[3]/div[2]/div')\n",
    "                                        #    /html/body/div[2]/main/section/div/div[3]/div[2]/div[1]\n",
    "    # print(len(elts))\n",
    "    # test = driver.find_element(By.CLASS_NAME, 'rating-item')\n",
    "    # print(len(test))\n",
    "    # return\n",
    "\n",
    "    # ------------- #\n",
    "    #     Title     #\n",
    "    # ------------- #\n",
    "    title = get_title(soup_movie)\n",
    "    print(\"Title:\" , title)\n",
    "\n",
    "    # --------------------------------- #\n",
    "    #   Date, duration and categories   #\n",
    "    # --------------------------------- #\n",
    "    date, duration, categories = get_date_duration_categories(soup_movie)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Duration:\", duration)\n",
    "    print(\"Categories:\", categories)\n",
    "    \n",
    "    if 'Documentaire' in categories:\n",
    "        print('Documentaire and more categories', categories)\n",
    "        # return (We do not scrap the documentaries ???)\n",
    "\n",
    "    # ------------------------------------------------- #\n",
    "    #   Directors list / Actors list / Composers list   #\n",
    "    # ------------------------------------------------- #    \n",
    "    lst_directors, lst_actors, lst_composers = [], [], []\n",
    "    elts_end_section = soup_movie.find_all('a', class_ = 'end-section-link')\n",
    "    \n",
    "    for elt_end_section in elts_end_section:\n",
    "        if 'Casting' in elt_end_section['title']:\n",
    "            # If there is a link to the whole casting\n",
    "            link_casting = elt_end_section['href']\n",
    "            r = requests.get(url_site + link_casting, auth=('user', 'pass'))\n",
    "            soup_casting = BeautifulSoup(r.content, 'html.parser')\n",
    "            # Get directors' list\n",
    "            lst_directors = get_directors(soup_casting)\n",
    "            # Get actors' list\n",
    "            lst_actors = get_actors(soup_casting)\n",
    "            # Composers' list\n",
    "            lst_composers = get_composers(soup_casting)\n",
    "\n",
    "        else: # No link to the casting (for example animation movies)\n",
    "            # print(elt_end_section.prettify())\n",
    "            elts = soup_movie.find_all('div', class_ = \"meta-body-item meta-body-direction meta-body-oneline\")\n",
    "            # Get directors' list\n",
    "            lst_directors = [elts[0].text.strip()[2:].strip()]\n",
    "            # For this kind of movies we only scrap one director (to see ...) and no actor, no composer\n",
    "\n",
    "    print('Directors:', lst_directors)\n",
    "    print('Actors:', lst_actors)\n",
    "    print('Composers:', lst_composers)\n",
    "\n",
    "    # ------------ #\n",
    "    #   Summary    #\n",
    "    # ------------ #\n",
    "    print(\"Summary:\", get_summary(soup_movie)[:180])\n",
    "\n",
    "    # ------------ #\n",
    "    #   thumbnail  #\n",
    "    # ------------ #\n",
    "    url_thumbnail = get_thumbnail(soup_movie)\n",
    "    print(\"Thumbnail:\", url_thumbnail)\n",
    "    # save_thumbnail(title, url_thumbnail)\n",
    "\n",
    "    # ------------ #\n",
    "    #    ratings   #\n",
    "    # ------------ #\n",
    "    get_ratings(soup_movie, driver)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------- #\n",
    "#                                    #\n",
    "#             Main loop              #\n",
    "#                                    #\n",
    "# ---------------------------------- #\n",
    "        \n",
    "\n",
    "# loop on all years to scrap (through links previously scrapped)\n",
    "# then loop on all pages of the year\n",
    "# then loop on all movies on one page\n",
    "# \n",
    "\n",
    "# delete_thumbnails()\n",
    "\n",
    "# Create Selenium driver\n",
    "# driver.close()\n",
    "driver = webdriver.Chrome(options = options)\n",
    "\n",
    "for year, url_year in dict_year_link.items():\n",
    "    print(\"Scraping year:\", year)\n",
    "    \n",
    "    r = requests.get(url_year, auth=('user', 'pass'))\n",
    "    if r.status_code != 200:\n",
    "        print(\"url_site error\")\n",
    "    \n",
    "    soup_year = BeautifulSoup(r.content, 'html.parser')\n",
    "    nb_pages = number_pages_per_year(soup_year)\n",
    "    # print(\"Nb pages :\", nb_pages)\n",
    "\n",
    "    for i in range(nb_pages): # Need to reduce as some movies are totaly unknown with very few informations about\n",
    "        url_year_page = url_year + f'?page={i+1}'\n",
    "        r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "        if r.status_code != 200:\n",
    "            print(\"url_site error\")\n",
    "        \n",
    "        soup_movies = BeautifulSoup(r.content, 'html.parser')\n",
    "        elt_movies = soup_movies.find_all('li', class_='mdl')\n",
    "        for elt_movie in elt_movies[:15]:\n",
    "            print('---------------------------------------------------------------- ')\n",
    "            scrap_movie(elt_movie, driver)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
