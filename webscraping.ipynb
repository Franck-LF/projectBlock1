{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Webscraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Présentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Webscraping du site [www.allocine.fr](https://www.allocine.fr/films/) avec Beautiful Soup et Selenium.\n",
    "\n",
    "Nous vérifions tout d'abord le fichier \"robots.txt\" pour voir si nous sommes autorisés à scraper le film.\n",
    "\n",
    "![robots.txt](images/robots.txt.png)\n",
    "\n",
    "Il n'y a pas de limitation pour notre tâche puisque nous travaillons sous l'url : <code>https://www.allocine.fr/film/</code><br>\n",
    "\n",
    "Ensuite nous allons sur le site allocine, catégorie **films** et nous allons scraper les informations à partir des menus déroulants sur la gauche (les catégories de films, les pays et ensuite nous scraperons les films par année).\n",
    "\n",
    "![filtres](images/filtresSMALL.png)\n",
    "### Sources :\n",
    "**Beautiful Soup** :\n",
    "[beautiful-soup-4](https://beautiful-soup-4.readthedocs.io/en/latest/)<br>\n",
    "[beautiful-soup-4.readthedocs.io](https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree)<br>\n",
    "\n",
    "**Selenium** :<br>\n",
    "[selenium-python.readthedocs.io](https://selenium-python.readthedocs.io/locating-elements.html)<br>\n",
    "[selenium.dev/documentation](https://www.selenium.dev/documentation/webdriver/elements/information/)<br>\n",
    "[selenium.dev/documentation/finders/](https://www.selenium.dev/documentation/webdriver/elements/finders/)<br>\n",
    "[geeksforgeeks.org/get_property-selenium/](https://www.geeksforgeeks.org/get_property-element-method-selenium-python/)<br>\n",
    "\n",
    "Les liens sont sûrement générés aléatoirement dynamiquement, on peut utiliser XPath avec selenium<br>\n",
    "ou bien avec lxml ??<br>\n",
    "\n",
    "Sur ce lien https://medium.com/swlh/web-scraping-using-selenium-and-beautifulsoup-adfc8810240a Selenium est utilisé pour faire le scraping des urls puis ensuite beautiful soup est utilisé pour faire le scraping des pages de chaque urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import re\n",
    "import io\n",
    "import math\n",
    "import copy\n",
    "import httpx\n",
    "import requests\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "url_site  = 'https://www.allocine.fr/'\n",
    "url_films = 'https://www.allocine.fr/films/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On scrape la liste des genres de film**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "Nb categories : 37\n"
     ]
    }
   ],
   "source": [
    "# Scrap all categories\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(type(soup))\n",
    "\n",
    "categories = []\n",
    "elt_categories = soup.find('div', class_='filter-entity-section')\n",
    "for elt in elt_categories.find_all('li'):\n",
    "    #print(elt.prettify())\n",
    "    categories.append(elt.a.text)\n",
    "\n",
    "print(\"Nb categories :\", len(categories))\n",
    "df_categories = pd.Series(categories)\n",
    "\n",
    "dict_n_cat = {k:v for k, v in enumerate(categories)}\n",
    "dict_cat_n = {v:k for k, v in dict_n_cat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On scrape la liste des pays d'origine des films**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'U.S.A.', 'Afrique du Sud', 'Albanie', 'Algérie', 'Allemagne', \"Allemagne de l'Est\", \"Allemagne de l'Ouest\", 'Arabie Saoudite', 'Argentine', 'Arménie', 'Australie', 'Autriche', 'Belgique', 'Bengladesh', 'Bolivie', 'Bosnie-Herzégovine', 'Brésil', 'Bulgarie', 'Burkina Faso', 'Cambodge', 'Cameroun', 'Canada', 'Chili', 'Chine', 'Chypre', 'Colombie', 'Corée', 'Corée du Sud', 'Croatie', 'Cuba', \"Côte-d'Ivoire\", 'Danemark', 'Egypte', 'Emirats Arabes Unis', 'Espagne', 'Estonie', 'Finlande', 'Grande-Bretagne', 'Grèce', 'Géorgie', 'Hong-Kong', 'Hongrie', 'Inde', 'Indonésie', 'Irak', 'Iran', 'Irlande', 'Islande', 'Israël', 'Italie', 'Japon', 'Jordanie', 'kazakhstan', 'Kenya', 'Kosovo', 'Lettonie', 'Liban', 'Lituanie', 'Luxembourg', 'Macédoine', 'Malaisie', 'Maroc', 'Mexique', 'Monténégro', 'Nigéria', 'Norvège', 'Nouvelle-Zélande', 'Pakistan', 'Palestine', 'Pays-Bas', 'Philippines', 'Pologne', 'Portugal', 'Pérou', 'Qatar', 'Roumanie', 'Russie', 'République dominicaine', 'République tchèque', 'Serbie', 'Singapour', 'Slovaquie', 'Slovénie', 'Sri Lanka', 'Suisse', 'Suède', 'Syrie', 'Sénégal', 'Taïwan', 'Tchécoslovaquie', 'Thaïlande', 'Tunisie', 'Turquie', 'Ukraine', 'URSS', 'Uruguay', 'Vietnam', 'Vénézuela', 'Yougoslavie']\n",
      "Nb pays : 100\n"
     ]
    }
   ],
   "source": [
    "# Scrap all countries\n",
    "elt_countries = elt_categories.find_next_sibling().find_next_sibling()\n",
    "elts_items = elt_countries.find_all('li', class_ = 'filter-entity-item')\n",
    "\n",
    "countries = []\n",
    "for elt_item in elts_items:\n",
    "    countries.append(elt_item.find('span').text.strip())\n",
    "\n",
    "print(countries)\n",
    "\n",
    "print(\"Nb pays :\", len(countries))\n",
    "df_countries = pd.Series(countries)\n",
    "\n",
    "dict_n_countries = {k:v for k, v in enumerate(countries)}\n",
    "dict_countries_n = {v:k for v, k in dict_n_countries.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On scrape les liens redirigeants vers les pages de films par année**\n",
    "\n",
    "**Nous utilisons Selenium**<br>\n",
    "Lors de l'utilisation de Beautiful Soup, certains éléments sont **décorés**, certains liens sont **invisibles**, on ne peut pas directement les scraper.<br>\n",
    "Le contournement trouvé est d'utiliser Selenium qui permet entre autre :\n",
    "- d'utiliser les XPath,\n",
    "- de récupérer tous les élements et non-décorés.\n",
    "\n",
    "On se donne une liste d'années, par exemple [1980, ... 2000]\n",
    "\n",
    "[1980 - 1989] puis [1990 - 1999] ... jusqu'à [2000 - 2009].<br>\n",
    "(cela fait plus de 40000 films et XXX reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Liens visualisés dans l'inspecteur html de Chrome**\n",
    "\n",
    "![links_decades_inspector](images/link_decades_inspector.png)\n",
    "\n",
    "\n",
    "**Liens visualisés avec Beautiful Soup**\n",
    "\n",
    "![link](images/link_decades_bs4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the limit of Beautiful Soup\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# print(soup.prettify())\n",
    "\n",
    "elt_decades = elt_categories.find_next_sibling()\n",
    "print(elt_decades.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:04<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1980   ----  link https://www.allocine.fr/films/decennie-1980/annee-1980/\n"
     ]
    }
   ],
   "source": [
    "# Scrap the links of the years\n",
    "\n",
    "# Input: list of years to scrap\n",
    "lst_years_to_scrap = list(range(1980, 2009))\n",
    "lst_years_to_scrap = [1980]\n",
    "\n",
    "lst_decades_to_scrap = list(set([10 * (year // 10) for year in lst_years_to_scrap]))\n",
    "lst_years_to_scrap = [str(year) for year in lst_years_to_scrap]\n",
    "lst_decades_to_scrap = [str(decade) for decade in lst_decades_to_scrap]\n",
    "\n",
    "driver = webdriver.Chrome(options = options)\n",
    "driver.get(url_films)\n",
    "elts_decades = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section[4]/div[1]/div/div[3]/div[2]/ul/li')\n",
    "\n",
    "dict_year_link = {}\n",
    "for elt_decade in tqdm(elts_decades):\n",
    "    elt_a = elt_decade.find_element(By.TAG_NAME, 'a')\n",
    "    if not(elt_a.get_attribute('title')[:4] in lst_decades_to_scrap):\n",
    "        continue\n",
    "\n",
    "    driver2 = webdriver.Chrome(options = options)\n",
    "    url_decade = elt_a.get_attribute('href').strip()\n",
    "\n",
    "    driver2.get(url_decade)\n",
    "    elts_years = driver2.find_elements(By.XPATH, '/html/body/div[2]/main/section[4]/div[1]/div/div[3]/div[3]/ul/li')\n",
    "\n",
    "    for elt_year in elts_years:\n",
    "        year = elt_year.find_element(By.TAG_NAME, 'a').get_attribute('title').strip()\n",
    "        if year in lst_years_to_scrap:\n",
    "            link = elt_year.find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n",
    "            dict_year_link[year] = link\n",
    "    driver2.close()\n",
    "\n",
    "for year, url_year in dict_year_link.items():\n",
    "    print(\"year\", year, '  ----  link', url_year)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On scrape les films à partir des liens vers les années**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pour scraper les directeurs et acteurs nous allons sur la page **casting** du film puis nous scrapons les acteurs principaux représentés dans la mosaïque, ensuite nous scrapons la liste des acteurs secondaires.\n",
    "\n",
    "![all_actors](images/scraping_all_actors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe comment récupérere l'url de la page des films similaires en fonction de la page du film :<br>\n",
    "https://www.allocine.fr/film/fichefilm_gen_cfilm=180.html<br>\n",
    "https://www.allocine.fr/film/fichefilm-180/similaire/<br>\n",
    "\n",
    "Cela suit toujours le même modèle, nous allons pouvoir automatiser cela sans scraper les urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping year: 1980\n",
      "---------------------------------------------------------------- \n",
      "Title: Elephant Man\n",
      "Date: 8 avril 1981\n",
      "Duration: 2h 05min\n",
      "Categories: Biopic,Drame\n",
      "Directors: ['David Lynch']\n",
      "Actors: ['photo de Anthony Hopkins', 'photo de John Hurt', 'photo de Anne Bancroft', 'photo de John Gielgud', 'photo de Wendy Hiller', 'photo de Freddie Jones', 'photo de Hannah Gordon', 'photo de Michael Elphick', 'Lesley Dunlop', 'Kenny Baker', 'John Standing', 'Dexter Fletcher', 'Phoebe Nicholls', 'Patsy Smart', 'Frederick Treves', 'Roy Evans', 'Tony London']\n",
      "Composers: ['John Morris (II)']\n",
      "Summary: Londres, 1884. Le chirurgien Frederick Treves découvre un homme complètement défiguré et difforme, devenu une attraction de foire. John Merrick, \" le monstre \", doit son nom de Ele\n",
      "Thumbnail: https://fr.web.img4.acsta.net/c_310_420/pictures/20/02/21/16/48/4302324.jpg\n",
      "https://www.allocine.fr/film/fichefilm-180/critiques/spectateurs/ 4,4 29319 689\n",
      "url_similar_movies: https://www.allocine.fr/film/fichefilm-180/similaire/\n",
      "---------------------------------------------------------------- \n",
      "Title: Cannibal Holocaust\n",
      "Date: 22 avril 1981\n",
      "Duration: 1h 26min\n",
      "Categories: Epouvante-horreur\n",
      "Directors: ['Ruggero Deodato']\n",
      "Actors: ['photo de Robert Kerman', 'photo de Francesca Ciardi', 'photo de Perry Pirkanen', 'photo de Luca Barbareschi', 'photo de Salvatore Basile', 'photo de Ricardo Fuentes', 'photo de Gabriel Yorke', 'photo de Paolo Paoloni']\n",
      "Composers: ['Riz Ortolani']\n",
      "Summary: Une équipe de journalistes composée de trois hommes et une femme se rend dans la jungle amazonienne à la recherche de vrais cannibales. Bientôt, la troupe ne donne plus aucun signe\n",
      "Thumbnail: https://fr.web.img3.acsta.net/c_310_420/medias/nmedia/18/65/67/95/18943780.jpg\n",
      "https://www.allocine.fr/film/fichefilm-52677/critiques/spectateurs/ 2,1 5792 548\n",
      "url_similar_movies: https://www.allocine.fr/film/fichefilm-52677/similaire/\n"
     ]
    }
   ],
   "source": [
    "def number_pages_per_year(soup_year):\n",
    "    ''' Return the number of pages for one year'''\n",
    "    pagination = soup_year.find('div', class_='pagination-item-holder')\n",
    "    nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "    return int(nb_pages)\n",
    "\n",
    "def delete_thumbnails():\n",
    "    '''Delete all files in thumbnail directory'''\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        files = os.listdir(folder_name)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_name, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        print(\"All files deleted successfully.\")\n",
    "    except OSError:\n",
    "        print(\"Error occurred while deleting files.\")\n",
    "\n",
    "def get_title(soup_movie):\n",
    "    ''' Return the title of the movie '''\n",
    "    return soup_movie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text\n",
    "\n",
    "def get_date_duration_categories(soup_movie):\n",
    "    ''' Return date, duration and categories (as string) of the movie '''\n",
    "    elt = soup_movie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "    s1, s2, s3 = text.split('|')\n",
    "    date = s1[:-8].strip()\n",
    "    duration = s2.strip()\n",
    "    categories = s3.strip()\n",
    "    return date, duration, categories\n",
    "\n",
    "def get_directors(soup_casting):\n",
    "    ''' Return list of directors '''\n",
    "    elt_director_section = soup_casting.find('section', class_='section casting-director')\n",
    "    elt_temp = elt_director_section.find_next()\n",
    "    elts_directors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_directors = [elt_director.find('a').text for elt_director in elts_directors]\n",
    "    return lst_directors\n",
    "\n",
    "def get_actors(soup_casting):\n",
    "    ''' Return list of actors (maximum 30) '''\n",
    "    elt_actor_section = soup_casting.find('section', class_ = 'section casting-actor')\n",
    "    if not(elt_actor_section):\n",
    "        return []\n",
    "    elt_temp = elt_actor_section.find_next()\n",
    "    # scrap main actors (maximum eight actors in the mosaic, see image above)\n",
    "    elts_actors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_actors = [elt_actor.find('figure').find('span')['title'] for elt_actor in elts_actors]\n",
    "    elts_actors = elt_actor_section.find_all('div', class_ = 'md-table-row')\n",
    "    # scrap list of actors below the mosaic (we scrap maximum of (8 + 22) 30 actors in total)\n",
    "    lst_actors.extend([elt_actor.find('a').text for elt_actor in elts_actors[:22] if elt_actor.find('a')])\n",
    "    return lst_actors\n",
    "\n",
    "def get_composers(soup_casting):\n",
    "    ''' Scrap the name(s) of the music composer(s) '''\n",
    "    elts_sections = soup_casting.find_all(\"div\", class_ = \"section casting-list-gql\")\n",
    "    for elt_section in elts_sections:\n",
    "        elt_title = elt_section.find('div', class_ = 'titlebar section-title').find('h2')\n",
    "        if 'Soundtrack' in elt_title.text:\n",
    "            lst_composers = []\n",
    "            elts_composers = elt_section.find_all('div', class_ = 'md-table-row')\n",
    "            for elt_composer in elts_composers:\n",
    "                elts_span = elt_composer.find_all('span')\n",
    "                if len(elts_span) > 1 and 'Compositeur' in elts_span[1].text.strip():\n",
    "                    lst_composers.append(elts_span[0].text.strip())\n",
    "            return lst_composers\n",
    "\n",
    "def get_summary(soup_movie):\n",
    "    elt = soup_movie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    return elt.find('p', class_ = 'bo-p').text.strip()\n",
    "\n",
    "def get_thumbnail(soup_movie):\n",
    "    elt = soup_movie.find('figure', class_ = 'thumbnail')\n",
    "    return elt.span.img['src']\n",
    "\n",
    "def save_thumbnail(title, url_thumbnail):\n",
    "    '''Save the thumbnail as image file in directory \"thumbnails\"'''\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        title2 = title.replace('-', '')\n",
    "        image_name = f\"thumbnail-{title2}.jpg\"\n",
    "        file = open(folder_name + image_name, \"wb\")\n",
    "        image = httpx.get(url_thumbnail)\n",
    "        file.write(image.content)\n",
    "        # Display thumbnail in Jupyter / console\n",
    "        img = Image.open(io.BytesIO(image.content))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        # To change resolution: https://www.geeksforgeeks.org/change-image-resolution-using-pillow-in-python/\n",
    "    except IOError:\n",
    "        print(\"Cannot read the file\")\n",
    "    finally:\n",
    "        file.close()\n",
    "\n",
    "\n",
    "\n",
    "def get_similar_movies(url_similar_movies):\n",
    "    print('url_similar_movies:', url_similar_movies)\n",
    "    pass\n",
    "\n",
    "def scrap_movie(elt_movie, driver):\n",
    "    ''' scrap all movie informations '''\n",
    "    \n",
    "    # get the movie soup\n",
    "    # print(url_site, elt_movie.h2.a.get('href'))\n",
    "    url_movie = url_site + elt_movie.h2.a.get('href')[1:]\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soup_movie = BeautifulSoup(r.content, 'html.parser')\n",
    "    # print(soup_movie.prettify())\n",
    "    # print(type(driver), url_movie)\n",
    "    driver.get(url_movie)\n",
    "    # elts = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section/div/div[3]/div[2]/div')\n",
    "                                        #    /html/body/div[2]/main/section/div/div[3]/div[2]/div[1]\n",
    "    # print(len(elts))\n",
    "    # test = driver.find_element(By.CLASS_NAME, 'rating-item')\n",
    "    # print(len(test))\n",
    "    # return\n",
    "\n",
    "    # ------------- #\n",
    "    #     Title     #\n",
    "    # ------------- #\n",
    "    title = get_title(soup_movie)\n",
    "    print(\"Title:\" , title)\n",
    "\n",
    "    # --------------------------------- #\n",
    "    #   Date, duration and categories   #\n",
    "    # --------------------------------- #\n",
    "    date, duration, categories = get_date_duration_categories(soup_movie)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Duration:\", duration)\n",
    "    print(\"Categories:\", categories)\n",
    "    \n",
    "    if 'Documentaire' in categories:\n",
    "        print('Documentaire and more categories', categories)\n",
    "        # return (We do not scrap the documentaries ???)\n",
    "\n",
    "    # ------------------------------------------------- #\n",
    "    #   Directors list / Actors list / Composers list   #\n",
    "    # ------------------------------------------------- #    \n",
    "    lst_directors, lst_actors, lst_composers = [], [], []\n",
    "    elts_end_section = soup_movie.find_all('a', class_ = 'end-section-link')\n",
    "    \n",
    "    for elt_end_section in elts_end_section:\n",
    "        if 'Casting' in elt_end_section['title']:\n",
    "            # If there is a link to the whole casting\n",
    "            link_casting = elt_end_section['href']\n",
    "            r = requests.get(url_site + link_casting, auth=('user', 'pass'))\n",
    "            soup_casting = BeautifulSoup(r.content, 'html.parser')\n",
    "            # Get directors' list\n",
    "            lst_directors = get_directors(soup_casting)\n",
    "            # Get actors' list\n",
    "            lst_actors = get_actors(soup_casting)\n",
    "            # Composers' list\n",
    "            lst_composers = get_composers(soup_casting)\n",
    "\n",
    "        else: # No link to the casting (for example animation movies)\n",
    "            # print(elt_end_section.prettify())\n",
    "            elts = soup_movie.find_all('div', class_ = \"meta-body-item meta-body-direction meta-body-oneline\")\n",
    "            # Get directors' list\n",
    "            lst_directors = [elts[0].text.strip()[2:].strip()]\n",
    "            # For this kind of movies we only scrap one director (to see ...) and no actor, no composer\n",
    "\n",
    "    print('Directors:', lst_directors)\n",
    "    print('Actors:', lst_actors)\n",
    "    print('Composers:', lst_composers)\n",
    "\n",
    "    # ------------ #\n",
    "    #   Summary    #\n",
    "    # ------------ #\n",
    "    print(\"Summary:\", get_summary(soup_movie)[:180])\n",
    "\n",
    "    # ------------ #\n",
    "    #   Thumbnail  #\n",
    "    # ------------ #\n",
    "    url_thumbnail = get_thumbnail(soup_movie)\n",
    "    print(\"Thumbnail:\", url_thumbnail)\n",
    "    # save_thumbnail(title, url_thumbnail)\n",
    "\n",
    "    # ------------ #\n",
    "    #    Ratings   #\n",
    "    # ------------ #\n",
    "    url_reviews = url_movie.replace('_gen_cfilm=', '-')[:-5] + '/critiques/spectateurs/'\n",
    "    url_reviews, stareval, notes, critics = get_ratings(soup_movie, driver, url_reviews, True)\n",
    "\n",
    "    # ------------------- #\n",
    "    #    Similar Movies   #\n",
    "    # ------------------- #\n",
    "    url_similar_movies = url_movie.replace('_gen_cfilm=', '-')[:-5] + '/similaire/' \n",
    "    get_similar_movies(url_similar_movies)\n",
    "    return\n",
    "\n",
    "def get_ratings(soup_movie, driver, url_reviews, use_Selenium):\n",
    "    ''' Scrap the ratings of the movie:\n",
    "\n",
    "        - star rating (0.5 to 5),\n",
    "        - number of votes,\n",
    "        - number of critics (written reviews).\n",
    "    '''\n",
    "\n",
    "    if use_Selenium:\n",
    "        elts_ratings = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "        url_reviews, stareval, notes, critics = '', '', '', ''\n",
    "\n",
    "        for elt_rating in elts_ratings:\n",
    "            elt = None\n",
    "            try:\n",
    "                elt = elt_rating.find_element(By.TAG_NAME, 'a')\n",
    "                if 'Spectateurs' in elt.text.strip():\n",
    "                    url_reviews = elt.get_attribute('href')\n",
    "                    elt_stareval_note = elt_rating.find_element(By.CLASS_NAME, 'stareval-note')\n",
    "                    stareval = elt_stareval_note.text.strip()\n",
    "                    elt_stareval_review = elt_rating.find_element(By.CLASS_NAME, 'stareval-review')\n",
    "                    stareval_review = elt_stareval_review.text.strip()\n",
    "                    assert stareval_review.count(',') == 1\n",
    "                    notes, critics = stareval_review.split(',')\n",
    "                    notes = notes.split()[0]\n",
    "                    critics = critics.split()[0]\n",
    "                    print(url_reviews, stareval, notes, critics)\n",
    "                    return url_reviews, stareval, notes, critics\n",
    "            except:\n",
    "                print('no tag \"a\" in elt_rating')\n",
    "\n",
    "    else: # user beautiful soup\n",
    "        # To Do with beautiful soup\n",
    "        \n",
    "        pass\n",
    "\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "# ---------------------------------- #\n",
    "#                                    #\n",
    "#             Main loop              #\n",
    "#                                    #\n",
    "# ---------------------------------- #\n",
    "        \n",
    "\n",
    "# loop on all years to scrap (through links previously scrapped)\n",
    "# then loop on all pages of the year\n",
    "# then loop on all movies on one page\n",
    "# \n",
    "\n",
    "# delete_thumbnails()\n",
    "\n",
    "# Create Selenium driver\n",
    "# driver.close()\n",
    "driver = webdriver.Chrome(options = options)\n",
    "\n",
    "for year, url_year in dict_year_link.items():\n",
    "    print(\"Scraping year:\", year)\n",
    "    \n",
    "    r = requests.get(url_year, auth=('user', 'pass'))\n",
    "    if r.status_code != 200:\n",
    "        print(\"url_site error\")\n",
    "    \n",
    "    soup_year = BeautifulSoup(r.content, 'html.parser')\n",
    "    nb_pages = number_pages_per_year(soup_year)\n",
    "    # print(\"Nb pages :\", nb_pages)\n",
    "\n",
    "    for i in range(nb_pages): # Need to reduce as some movies are totaly unknown with very few informations about\n",
    "        url_year_page = url_year + f'?page={i+1}'\n",
    "        r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "        if r.status_code != 200:\n",
    "            print(\"url_site error\")\n",
    "        \n",
    "        soup_movies = BeautifulSoup(r.content, 'html.parser')\n",
    "        elt_movies = soup_movies.find_all('li', class_='mdl')\n",
    "        for elt_movie in elt_movies[:2]:\n",
    "            print('---------------------------------------------------------------- ')\n",
    "            scrap_movie(elt_movie, driver)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
