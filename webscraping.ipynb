{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> webscraping\n",
    "\n",
    "### première partie du projet bloc 1\n",
    "\n",
    "- Webscraping du site [www.allocine.fr](https://www.allocine.fr/films/)\n",
    "\n",
    "![filtres](images/filtresSMALL.png)\n",
    "\n",
    "## Sources :\n",
    "**Beautiful Soup** :\n",
    "[beautiful-soup-4](https://beautiful-soup-4.readthedocs.io/en/latest/)<br>\n",
    "[beautiful-soup-4.readthedocs.io](https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree)<br>\n",
    "\n",
    "**Selenium** :<br>\n",
    "[selenium-python.readthedocs.io](https://selenium-python.readthedocs.io/locating-elements.html)<br>\n",
    "[selenium.dev/documentation](https://www.selenium.dev/documentation/webdriver/elements/information/)<br>\n",
    "[selenium.dev/documentation/finders/](https://www.selenium.dev/documentation/webdriver/elements/finders/)<br>\n",
    "[geeksforgeeks.org/get_property-selenium/](https://www.geeksforgeeks.org/get_property-element-method-selenium-python/)<br>\n",
    "\n",
    "Les liens sont sûrement générés aléatoirement dynamiquement, on peut utiliser XPath avec selenium<br>\n",
    "ou bien avec lxml ??<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "#import httpx\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options = options)\n",
    "\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "url_site = 'https://www.allocine.fr/'\n",
    "url_films = 'https://www.allocine.fr/films/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape tous les genres de film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "Nb categories : 37\n"
     ]
    }
   ],
   "source": [
    "# Scrap all categories\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(type(soup))\n",
    "\n",
    "categories = []\n",
    "elt_categories = soup.find('div', class_='filter-entity-section')\n",
    "for elt in elt_categories.find_all('li'):\n",
    "    #print(elt.prettify())\n",
    "    categories.append(elt.a.text)\n",
    "\n",
    "print(\"Nb categories :\", len(categories))\n",
    "df_categories = pd.Series(categories)\n",
    "\n",
    "dict_n_cat = {k:v for k, v in enumerate(categories)}\n",
    "dict_cat_n = {v:k for v, k in dict_n_cat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrapte les films par période\n",
    "[1980 - 1989] puis [1990 - 1999] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1980']\n",
      "2030 - 2039\n",
      "2020 - 2029\n",
      "2010 - 2019\n",
      "2000 - 2009\n",
      "1990 - 1999\n",
      "** 1980 - 1989\n",
      "1970 - 1979\n",
      "1960 - 1969\n",
      "1950 - 1959\n",
      "1940 - 1949\n",
      "1930 - 1939\n",
      "1920 - 1929\n",
      "1910 - 1919\n",
      "1900 - 1909\n",
      "1890 - 1899\n"
     ]
    }
   ],
   "source": [
    "# Scrap url of years we want to scrap the movies\n",
    "# Not Working\n",
    "# I cannot get the url by scraping\n",
    "decades_to_scrap = list([str(item) for item in range(1980, 2020, 10)])\n",
    "decades_to_scrap = list([str(item) for item in range(1980, 1989, 10)])\n",
    "print(decades_to_scrap)\n",
    "elt_years = elt_categories.find_next_sibling()\n",
    "#print(eltYears.prettify())\n",
    "lstUrl = []\n",
    "elt_cur = elt_years.find('li')\n",
    "\n",
    "while elt_cur:\n",
    "    text = elt_cur.span.text\n",
    "    #print(elt_cur)\n",
    "    #print(elt_cur.span.get('href')) # NOT WORKING because url are 'decorated'\n",
    "    if text[:4] in decades_to_scrap:\n",
    "        print(\"**\", text)\n",
    "    else:\n",
    "        print(elt_cur.span.text)\n",
    "    elt_cur = elt_cur.find_next_sibling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autre méthode utilisant Selenium\n",
    "Puisque nous n'arrivons pas à récupérer les urls nous allons utiliser Selenium qui permet entre autre :\n",
    "- d'utiliser les XPath (contrairement à Beautiful Soup)\n",
    "- de récupérer certains élements 'décorés' par exemple des urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980 - 1989\n",
      "decade: https://www.allocine.fr/films/decennie-1980/\n",
      "10\n",
      "year: https://www.allocine.fr/films/decennie-1980/annee-1980/\n",
      "Title: La Boum\n",
      "Date: 17 décembre 1980\n",
      "Duration: 1h 49min\n",
      "Categories: Comédie,Drame,Romance\n",
      "/film/fichefilm-4403/casting/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 159\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m#print(\"nb movies per page:\", len(elt_movies))\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elt_movie \u001b[38;5;129;01min\u001b[39;00m elt_movies:\n\u001b[1;32m--> 159\u001b[0m     \u001b[43mscrap_movie\u001b[49m\u001b[43m(\u001b[49m\u001b[43melt_movie\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[78], line 51\u001b[0m, in \u001b[0;36mscrap_movie\u001b[1;34m(elt_movie)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategories:\u001b[39m\u001b[38;5;124m\"\u001b[39m, categories)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Get directors / main director, sub directors ...???? \u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mget_directors_and_actors\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup_movie\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     54\u001b[0m elts \u001b[38;5;241m=\u001b[39m soup_movie\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-body-item meta-body-direction meta-body-oneline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[78], line 26\u001b[0m, in \u001b[0;36mget_directors_and_actors\u001b[1;34m(soup_movie)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(soup_casting.prettify())\u001b[39;00m\n\u001b[0;32m     25\u001b[0m elt_director \u001b[38;5;241m=\u001b[39m soup_casting\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msectioncasting-director\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m elts \u001b[38;5;241m=\u001b[39m \u001b[43melt_director\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdic\u001b[39m\u001b[38;5;124m'\u001b[39m, class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson-card\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elts))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "def numberPagesPerYear(soup_year):\n",
    "    ''' Return the number of pages for one year'''\n",
    "    pagination = soup_year.find('div', class_='pagination-item-holder')\n",
    "    nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "    return int(nb_pages)\n",
    "\n",
    "def get_title(soup_movie):\n",
    "    return soup_movie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text\n",
    "\n",
    "def get_date_duration_categories(soup_movie):\n",
    "    elt = soup_movie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "    s1, s2, s3 = text.split('|')\n",
    "    date = s1[:-8].strip()\n",
    "    duration = s2.strip()\n",
    "    categories = s3.strip()\n",
    "    return date, duration, categories\n",
    "\n",
    "def get_directors_and_actors(soup_movie):\n",
    "    link_casting = soup_movie.find('a', class_ = 'end-section-link')['href']\n",
    "    print(link_casting)\n",
    "    r = requests.get(url_site + link_casting, auth=('user', 'pass'))\n",
    "    soup_casting = BeautifulSoup(r.content, 'html.parser')\n",
    "    # print(soup_casting.prettify())\n",
    "    elt_director = soup_casting.find('section', class_='section-casting-director')\n",
    "    elts = elt_director.find_all('dic', class_ = 'person-card')\n",
    "    print(len(elts))\n",
    "\n",
    "\n",
    "def get_thumbnail(soup_movie):\n",
    "    elt = soup_movie.find('figure', class_ = 'thumbnail')\n",
    "    return elt.span.img['src']\n",
    "\n",
    "def scrap_movie(elt_movie):\n",
    "    ''' scrap all movie informations '''   \n",
    "    # get soup\n",
    "    url_movie = url_site + elt_movie.h2.a.get('href')\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soup_movie = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    # Get title\n",
    "    print(\"Title:\" , get_title(soup_movie))\n",
    "\n",
    "    # Get date, duration and categories\n",
    "    date, duration, categories = get_date_duration_categories(soup_movie)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Duration:\", duration)\n",
    "    print(\"Categories:\", categories)\n",
    "\n",
    "    # Get directors / main director, sub directors ...???? \n",
    "    get_directors_and_actors(soup_movie)\n",
    "    return\n",
    "\n",
    "    elts = soup_movie.find_all('div', class_ = \"meta-body-item meta-body-direction meta-body-oneline\")\n",
    "    #assert len(elts)\n",
    "    directors = [elts[0].text.strip()[2:].strip()]\n",
    "\n",
    "    if len(elts) > 1:\n",
    "        elts_span = elts[1].find_all('span')\n",
    "        for elt in elts_span:\n",
    "            if 'light' in elt['class']:\n",
    "                continue\n",
    "            if elt.get_text(strip=True) not in directors:\n",
    "                directors.append(elt.get_text(strip=True))\n",
    "    directors = ', '.join(directors)\n",
    "    print(\"Directors:\", directors)\n",
    "\n",
    "    # Get actors\n",
    "    elt = soup_movie.find('div', class_ = \"meta-body-item meta-body-actor\")\n",
    "    actors = elt.get_text(strip=True)[4:]\n",
    "    print(\"Actors:\", actors)\n",
    "\n",
    "    # Get summary\n",
    "    elt = soup_movie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    elt2 = elt.find('div', class_ = \"content-txt\")\n",
    "    elt3 = elt.find('p', class_ = 'bo-p')\n",
    "    summary = elt3.text.strip()\n",
    "    print(\"Summary:\", summary)\n",
    "\n",
    "    # Get thumbnail url\n",
    "    url_thumbnail = get_thumbnail(soup_movie)\n",
    "    return\n",
    "\n",
    "    # Get ratings\n",
    "\n",
    "    # driver to the movie page\n",
    "    driver.get(url_movie)\n",
    "    elts_rating = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "    print(len(elts))\n",
    "    for elt in elts:\n",
    "        #print(elt.text)\n",
    "        elt_a = elt.find_element(By.TAG_NAME, 'a')\n",
    "        print(elt_a.get_attribute('href'))\n",
    "\n",
    "    return\n",
    "    # beautiful soup version\n",
    "    elts_rating = soupMovie.find_all('div', class_ = 'rating-item')\n",
    "    #print(len(elts_rating))\n",
    "    ratings = {}\n",
    "\n",
    "    for elt_rating in elts_rating:\n",
    "        print(elt_rating.prettify())\n",
    "        elt_temp = elt_rating.find('div', class_='rating-item-content')\n",
    "        elt_span = elt_temp.find('span')\n",
    "        print(\"span class\", elt_span['class'])\n",
    "        if 'rating-title' in elt_span['class']:\n",
    "            print('rating-title')\n",
    "            print(elt_span.get_text(strip = True))\n",
    "            if 'Spectateurs' in elt_span.get_text(strip = True):\n",
    "                ratings['spectateurs'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "        #     # print(elt_span.get_text(strip = True))\n",
    "        # elif 'rating-title' in elt_span['class']:\n",
    "        #     if 'Presse' in elt_span.get_text(strip = True):\n",
    "        #         ratings['Presse'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "    print(\"ratings:\", ratings)\n",
    "\n",
    "\n",
    "driver.get(url_films)\n",
    "elts_decades = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section[3]/div[1]/div/div[3]/div[2]/ul/li')\n",
    "\n",
    "for elt_decade in elts_decades:\n",
    "    elt_a = elt_decade.find_element(By.TAG_NAME, 'a')\n",
    "    if elt_a.get_attribute('title')[:4] in decades_to_scrap:\n",
    "        print(elt_a.get_attribute('title'))\n",
    "    \n",
    "        url_decades = elt_a.get_attribute('href')\n",
    "        print(\"decade:\", url_decades)\n",
    "        driver2 = webdriver.Chrome(options = options)\n",
    "        driver2.get(url_decades)\n",
    "\n",
    "        elts_years = driver2.find_elements(By.XPATH, '/html/body/div[2]/main/section[3]/div[1]/div/div[3]/div[3]/ul/li')\n",
    "        print(len(elts_years))\n",
    "        \n",
    "        for elt_year in elts_years[::-1]:\n",
    "            elt_a_year = elt_year.find_element(By.TAG_NAME, 'a')\n",
    "\n",
    "            url_year = elt_a_year.get_attribute('href')\n",
    "            print(\"year:\", url_year)\n",
    "\n",
    "            r = requests.get(url_year, auth=('user', 'pass'))\n",
    "            if r.status_code != 200:\n",
    "                print(\"url_site error\")\n",
    "\n",
    "            # We get the number of pages for this year\n",
    "            soup_year = BeautifulSoup(r.content, 'html.parser')\n",
    "            nb_pages = numberPagesPerYear(soup_year)\n",
    "            # print('Nb pages:', nb_pages)\n",
    "\n",
    "            for i in range(nb_pages): # Need to reduce as some movies are totaly unknown with very few informations about\n",
    "                url_year_page = url_year + f'?page={i+1}'\n",
    "                # print(url_year_page)\n",
    "                r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "                if r.status_code != 200:\n",
    "                    print(\"url_site error\")\n",
    "                soup_movies = BeautifulSoup(r.content, 'html.parser')\n",
    "                elt_movies = soup_movies.find_all('li', class_='mdl')\n",
    "                #print(\"nb movies per page:\", len(elt_movies))\n",
    "                for elt_movie in elt_movies:\n",
    "                    scrap_movie(elt_movie)\n",
    "                    break\n",
    "                break\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ou bien nous pouvons entrer les url manuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "url_decades = url_films + 'decennie-1980/'\n",
    "url_year = url_films + 'decennie-1980/annee-1980/'\n",
    "\n",
    "def getNumberOfPages(elt):\n",
    "    nb = 0\n",
    "    while elt:\n",
    "        if elt.text.isdigit():\n",
    "            nb = elt.text\n",
    "        elt = elt.find_next_sibling()\n",
    "    return int(nb)\n",
    "\n",
    "r = requests.get(url_year, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "\n",
    "# We get the number of pages for this year\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "pagination = soup.find('div', class_='pagination-item-holder')\n",
    "nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "assert nb_pages == getNumberOfPages(pagination.find('span'))\n",
    "print(nb_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape chaque page de films pour une année donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: La Boum\n",
      "Date: 17 décembre 1980\n",
      "Duration: 1h 49min\n",
      "Categories: Comédie,Drame,Romance\n",
      "Authors: Claude Pinoteau, Danièle Thompson\n",
      "Actors: Sophie Marceau,Brigitte Fossey,Claude Brasseur\n",
      "Summary: Vic vit tranquillement entre le lycée, ses parents et Poupette, son arrière-grand-mère. Lorsque sa mère apprend l'existence d'une ancienne maîtresse de son mari, elle décide de \"faire un break\" mais du haut de ses 13 ans Vic ne pense qu'à sa première boum...\n",
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 85\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eltMovie \u001b[38;5;129;01min\u001b[39;00m eltMovies:\n\u001b[0;32m     84\u001b[0m     url_movie \u001b[38;5;241m=\u001b[39m url_site \u001b[38;5;241m+\u001b[39m eltMovie\u001b[38;5;241m.\u001b[39mh2\u001b[38;5;241m.\u001b[39ma\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mscrapMoviePage\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_movie\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#    print()\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#break\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[160], line 48\u001b[0m, in \u001b[0;36mscrapMoviePage\u001b[1;34m(url_movie)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elts))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m elts:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m#print(elt.text)\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     elt_a \u001b[38;5;241m=\u001b[39m \u001b[43melt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTAG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(elt_a\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "def scrapMoviePage(url_movie):\n",
    "    # get soup from movie page\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soupMovie = BeautifulSoup(r.content, 'html.parser')\n",
    "    print(\"Title:\" , soupMovie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text)\n",
    "    \n",
    "    elt = soupMovie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "\n",
    "    s1, s2, s3 = text.split('|')\n",
    "    date = s1[:-8].strip()\n",
    "    print(\"Date:\", date)\n",
    "    duration = s2.strip()\n",
    "    print(\"Duration:\", duration)\n",
    "    categories = s3.strip()\n",
    "    print(\"Categories:\", categories)\n",
    "\n",
    "    elts = soupMovie.find_all('div', class_ = \"meta-body-item meta-body-direction meta-body-oneline\")\n",
    "    #assert len(elts)\n",
    "    authors = [elts[0].text.strip()[2:].strip()]\n",
    "\n",
    "    if len(elts) > 1:\n",
    "        elts_span = elts[1].find_all('span')\n",
    "        for elt in elts_span:\n",
    "            if 'light' in elt['class']:\n",
    "                continue\n",
    "            if elt.get_text(strip=True) not in authors:\n",
    "                authors.append(elt.get_text(strip=True))\n",
    "    authors = ', '.join(authors)\n",
    "    print(\"Authors:\", authors)\n",
    "\n",
    "    elt = soupMovie.find('div', class_ = \"meta-body-item meta-body-actor\")\n",
    "    actors = elt.get_text(strip=True)[4:]\n",
    "    print(\"Actors:\", actors)\n",
    "\n",
    "    elt = soupMovie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    elt2 = elt.find('div', class_ = \"content-txt\")\n",
    "    elt3 = elt.find('p', class_ = 'bo-p')\n",
    "    summary = elt3.text.strip()\n",
    "    print(\"Summary:\", summary)\n",
    "\n",
    "    # driver to the movie page\n",
    "    driver.get(url_movie)\n",
    "    elts_rating = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "    print(len(elts))\n",
    "    for elt in elts:\n",
    "        #print(elt.text)\n",
    "        elt_a = elt.find_element(By.TAG_NAME, 'a')\n",
    "        print(elt_a.get_attribute('href'))\n",
    "\n",
    "    return\n",
    "    # beautiful soup version\n",
    "    elts_rating = soupMovie.find_all('div', class_ = 'rating-item')\n",
    "    #print(len(elts_rating))\n",
    "    ratings = {}\n",
    "\n",
    "    for elt_rating in elts_rating:\n",
    "        print(elt_rating.prettify())\n",
    "        elt_temp = elt_rating.find('div', class_='rating-item-content')\n",
    "        elt_span = elt_temp.find('span')\n",
    "        print(\"span class\", elt_span['class'])\n",
    "        if 'rating-title' in elt_span['class']:\n",
    "            print('rating-title')\n",
    "            print(elt_span.get_text(strip = True))\n",
    "            if 'Spectateurs' in elt_span.get_text(strip = True):\n",
    "                ratings['spectateurs'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "        #     # print(elt_span.get_text(strip = True))\n",
    "        # elif 'rating-title' in elt_span['class']:\n",
    "        #     if 'Presse' in elt_span.get_text(strip = True):\n",
    "        #         ratings['Presse'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "    print(\"ratings:\", ratings)\n",
    "\n",
    "movies = []\n",
    "\n",
    "for i in range(1, 3):\n",
    "    url_year_page = url_year + '?page=' + str(i)\n",
    "    r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "    if r.status_code != 200:\n",
    "        print(\"url_site error\")\n",
    "    soupMovies = BeautifulSoup(r.content, 'html.parser')\n",
    "    eltMovies = soupMovies.find_all('li', class_='mdl')\n",
    "\n",
    "    for eltMovie in eltMovies:\n",
    "        url_movie = url_site + eltMovie.h2.a.get('href')\n",
    "        scrapMoviePage(url_movie)\n",
    "    #    print()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape les pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltCountries = eltYears.find_next_sibling()\n",
    "print(eltCountries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Spectateurs\n",
      "3,0\n",
      "18413 notes, 225 critiques\n",
      "https://www.allocine.fr/film/fichefilm-4403/critiques/spectateurs/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "driver.get(\"https://www.allocine.fr/film/fichefilm_gen_cfilm=4403.html\")\n",
    "\n",
    "# elem = driver.find_element(By.NAME, \"q\")\n",
    "# elem.clear()\n",
    "# elem.send_keys(\"pycon\")\n",
    "# elem.send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "# elts = driver.find_elements(By.CLASS_NAME, 'xXx rating-title')\n",
    "elts = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "print(len(elts))\n",
    "for elt in elts[:1]:\n",
    "    print(elt.text)\n",
    "    elt_a = elt.find_element(By.TAG_NAME, 'a')\n",
    "    print(elt_a.get_attribute('href'))\n",
    "\n",
    "#elts_rating = elt.find_elements(By.XPATH, '/html/body/div[2]/main/section/div/div[3]/div[2]/div')\n",
    "# for elt in elts_rating:\n",
    "#     elt_temp = elt.find_element(By.CLASS_NAME, 'rating-item-content')\n",
    "#     elt_a = driver.find_element(By.TAG_NAME, 'a')\n",
    "    # elt_a = elt_temp.find_element(By.CLASS_NAME, 'xXx')\n",
    "    # elt_a = elt_temp.find_element(By.CLASS_NAME, 'rating-title')\n",
    "    # elt_a = elt.find_element(By.CLASS_NAME, 'xXx rating-title')\n",
    "    # print(elt_a.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
