{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> webscraping\n",
    "\n",
    "### première partie du projet bloc 1\n",
    "\n",
    "- Webscraping du site [www.allocine.fr](https://www.allocine.fr/films/)\n",
    "\n",
    "![filtres](images/filtresSMALL.png)\n",
    "\n",
    "## Sources :\n",
    "**Beautiful Soup** :\n",
    "[beautiful-soup-4](https://beautiful-soup-4.readthedocs.io/en/latest/)<br>\n",
    "[beautiful-soup-4.readthedocs.io](https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree)<br>\n",
    "\n",
    "**Selenium** :<br>\n",
    "[selenium-python.readthedocs.io](https://selenium-python.readthedocs.io/locating-elements.html)<br>\n",
    "[selenium.dev/documentation](https://www.selenium.dev/documentation/webdriver/elements/information/)<br>\n",
    "[selenium.dev/documentation/finders/](https://www.selenium.dev/documentation/webdriver/elements/finders/)<br>\n",
    "[geeksforgeeks.org/get_property-selenium/](https://www.geeksforgeeks.org/get_property-element-method-selenium-python/)<br>\n",
    "\n",
    "Les liens sont sûrement générés aléatoirement dynamiquement, on peut utiliser XPath avec selenium<br>\n",
    "ou bien avec lxml ??<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import httpx\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options = options)\n",
    "\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "url_site = 'https://www.allocine.fr/'\n",
    "url_films = 'https://www.allocine.fr/films/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape tous les genres de film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "Nb categories : 37\n"
     ]
    }
   ],
   "source": [
    "# Scrap all categories\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(type(soup))\n",
    "\n",
    "categories = []\n",
    "elt_categories = soup.find('div', class_='filter-entity-section')\n",
    "for elt in elt_categories.find_all('li'):\n",
    "    #print(elt.prettify())\n",
    "    categories.append(elt.a.text)\n",
    "\n",
    "print(\"Nb categories :\", len(categories))\n",
    "df_categories = pd.Series(categories)\n",
    "\n",
    "dict_n_cat = {k:v for k, v in enumerate(categories)}\n",
    "dict_cat_n = {v:k for v, k in dict_n_cat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape les films par période\n",
    "[1980 - 1989] puis [1990 - 1999] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1980']\n",
      "2030 - 2039\n",
      "2020 - 2029\n",
      "2010 - 2019\n",
      "2000 - 2009\n",
      "1990 - 1999\n",
      "** 1980 - 1989\n",
      "1970 - 1979\n",
      "1960 - 1969\n",
      "1950 - 1959\n",
      "1940 - 1949\n",
      "1930 - 1939\n",
      "1920 - 1929\n",
      "1910 - 1919\n",
      "1900 - 1909\n",
      "1890 - 1899\n"
     ]
    }
   ],
   "source": [
    "# Scrap url of years we want to scrap the movies\n",
    "# Not Working\n",
    "# I cannot get the url by scraping\n",
    "decades_to_scrap = list([str(item) for item in range(1980, 2020, 10)])\n",
    "decades_to_scrap = list([str(item) for item in range(1980, 1989, 10)])\n",
    "print(decades_to_scrap)\n",
    "elt_years = elt_categories.find_next_sibling()\n",
    "#print(eltYears.prettify())\n",
    "lstUrl = []\n",
    "elt_cur = elt_years.find('li')\n",
    "\n",
    "while elt_cur:\n",
    "    text = elt_cur.span.text\n",
    "    #print(elt_cur)\n",
    "    #print(elt_cur.span.get('href')) # NOT WORKING because url are 'decorated'\n",
    "    if text[:4] in decades_to_scrap:\n",
    "        print(\"**\", text)\n",
    "    else:\n",
    "        print(elt_cur.span.text)\n",
    "    elt_cur = elt_cur.find_next_sibling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autre méthode utilisant Selenium\n",
    "Puisque nous n'arrivons pas à récupérer les urls nous allons utiliser Selenium qui permet entre autre :\n",
    "- d'utiliser les XPath (contrairement à Beautiful Soup)\n",
    "- de récupérer certains élements 'décorés' par exemple des urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decade: https://www.allocine.fr/films/decennie-1980/\n",
      "year: https://www.allocine.fr/films/decennie-1980/annee-1980/\n",
      "Title: La Boum\n",
      "Date: 17 décembre 1980\n",
      "Duration: 1h 49min\n",
      "Categories: Comédie,Drame,Romance\n",
      "Directors: ['Claude Pinoteau']\n",
      "Actors: ['Sophie Marceau', 'Brigitte Fossey', 'Claude Brasseur', 'Denise Grey', \"Sheila O'Connor\", 'Bernard Giraudeau', 'Dominique Lavanant', 'Jean-Pierre Castaldi']\n",
      "Summary: Vic vit tranquillement entre le lycée, ses parents et Poupette, son arrière-grand-mère. Lorsque sa mère apprend l'existence d'une ancienne maîtresse de son mari, elle décide de \"faire un break\" mais du haut de ses 13 ans Vic ne pense qu'à sa première boum...\n",
      "Thumbnail: https://fr.web.img3.acsta.net/c_310_420/medias/nmedia/18/62/90/68/18658418.jpg\n",
      "c:\\Users\\Utilisateur\\Documents\\Block1\\thumbnails\\\n"
     ]
    }
   ],
   "source": [
    "def number_pages_per_year(soup_year):\n",
    "    ''' Return the number of pages for one year'''\n",
    "    pagination = soup_year.find('div', class_='pagination-item-holder')\n",
    "    nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "    return int(nb_pages)\n",
    "\n",
    "def get_title(soup_movie):\n",
    "    return soup_movie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text\n",
    "\n",
    "def get_date_duration_categories(soup_movie):\n",
    "    elt = soup_movie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "    s1, s2, s3 = text.split('|')\n",
    "    date = s1[:-8].strip()\n",
    "    duration = s2.strip()\n",
    "    categories = s3.strip()\n",
    "    return date, duration, categories\n",
    "\n",
    "def get_directors(soup_casting):\n",
    "    elt_director_section = soup_casting.find('section', class_='section casting-director')\n",
    "    elt_temp = elt_director_section.find_next()\n",
    "    # print(elt_temp.prettify())\n",
    "    elts_directors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_directors = [elt_director.find('a').text for elt_director in elts_directors]\n",
    "    return lst_directors\n",
    "\n",
    "def get_actors(soup_casting):\n",
    "    elt_actor_section = soup_casting.find('section', class_ = 'section casting-actor')\n",
    "    if not(elt_actor_section):\n",
    "        return []\n",
    "    elt_temp = elt_actor_section.find_next()\n",
    "    #print(elt_temp.prettify())\n",
    "    elts_actors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_actors = [elt_actor.find('figure').find('span')['title'] for elt_actor in elts_actors]\n",
    "    elts_actors = elt_actor_section.find_all('div', class_ = 'md-table-row')\n",
    "    #print('Actors:', elts_actors)\n",
    "    # We complete the actor list to get 10 actors\n",
    "    nb_complete = 10 - len(lst_actors)\n",
    "    #lst_actors.extend([elt_actor.find('a').text for elt_actor in elts_actors[:nb_complete] if elt_actor.find('a')])\n",
    "    return lst_actors\n",
    "\n",
    "def get_summary(soup_movie):\n",
    "    elt = soup_movie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    return elt.find('p', class_ = 'bo-p').text.strip()\n",
    "\n",
    "def get_thumbnail(soup_movie):\n",
    "    elt = soup_movie.find('figure', class_ = 'thumbnail')\n",
    "    return elt.span.img['src']\n",
    "\n",
    "def scrap_movie(elt_movie):\n",
    "    ''' scrap all movie informations '''\n",
    "    # get soup\n",
    "    url_movie = url_site + elt_movie.h2.a.get('href')\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soup_movie = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    # Get title\n",
    "    title = get_title(soup_movie)\n",
    "    print(\"Title:\" , get_title(soup_movie))\n",
    "\n",
    "    # Get date, duration and categories\n",
    "    date, duration, categories = get_date_duration_categories(soup_movie)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Duration:\", duration)\n",
    "    print(\"Categories:\", categories)\n",
    "\n",
    "    # Get directors / Actors\n",
    "    lst_directors, lst_actors = [], []\n",
    "    elt_link_casting = soup_movie.find('a', class_ = 'end-section-link')\n",
    "\n",
    "    if elt_link_casting and 'Casting' in elt_link_casting['title']:\n",
    "        # if title == 'Le Roi et l\\'oiseau':\n",
    "        #     print(elt_link_casting.prettify())\n",
    "        link_casting = elt_link_casting['href']\n",
    "        r = requests.get(url_site + link_casting, auth=('user', 'pass'))\n",
    "        soup_casting = BeautifulSoup(r.content, 'html.parser')\n",
    "        # Get directors' list\n",
    "        lst_directors = get_directors(soup_casting)\n",
    "        # Get actors' list\n",
    "        lst_actors = get_actors(soup_casting)\n",
    "\n",
    "    else: # Animation Case Only ???\n",
    "        # print(elt_link_casting.prettify())\n",
    "        elts = soup_movie.find_all('div', class_ = \"meta-body-item meta-body-direction meta-body-oneline\")\n",
    "        # Get directors' list\n",
    "        lst_directors = [elts[0].text.strip()[2:].strip()]\n",
    "\n",
    "        if len(elts) > 1:\n",
    "            elts_span = elts[1].find_all('span')\n",
    "            for elt in elts_span:\n",
    "                if 'light' in elt['class']:\n",
    "                    continue\n",
    "                if elt.get_text(strip=True) not in lst_directors:\n",
    "                    lst_directors.append(elt.get_text(strip=True))\n",
    "\n",
    "        # Get actors' list (Not for animation movie)\n",
    "        # elt = soup_movie.find('div', class_ = \"meta-body-item meta-body-actor\")\n",
    "        # lst_actors = elt.get_text(strip=True)[4:]\n",
    "\n",
    "    print('Directors:', lst_directors)\n",
    "    print('Actors:', lst_actors)\n",
    "\n",
    "    # Get summary\n",
    "    print(\"Summary:\", get_summary(soup_movie))\n",
    "\n",
    "    # Get thumbnail url\n",
    "    url_thumbnail = get_thumbnail(soup_movie)\n",
    "    print(\"Thumbnail:\", url_thumbnail)\n",
    "\n",
    "    # Save the image\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        print(folder_name)\n",
    "        image_name = f\"thumbnail-{title}.jpg\"\n",
    "        file = open(folder_name + image_name, \"wb\")\n",
    "        image = httpx.get(url_thumbnail)\n",
    "        file.write(image.content)\n",
    "    except IOError:\n",
    "        print(\"Cannot read the file\")\n",
    "    # finally:\n",
    "        # file.close()\n",
    "\n",
    "    return\n",
    "\n",
    "    # Get ratings\n",
    "\n",
    "    # driver to the movie page\n",
    "    driver.get(url_movie)\n",
    "    elts_rating = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "    print(len(elts))\n",
    "    for elt in elts:\n",
    "        #print(elt.text)\n",
    "        elt_a = elt.find_element(By.TAG_NAME, 'a')\n",
    "        print(elt_a.get_attribute('href'))\n",
    "\n",
    "    return\n",
    "    # beautiful soup version\n",
    "    elts_rating = soupMovie.find_all('div', class_ = 'rating-item')\n",
    "    #print(len(elts_rating))\n",
    "    ratings = {}\n",
    "\n",
    "    for elt_rating in elts_rating:\n",
    "        print(elt_rating.prettify())\n",
    "        elt_temp = elt_rating.find('div', class_='rating-item-content')\n",
    "        elt_span = elt_temp.find('span')\n",
    "        print(\"span class\", elt_span['class'])\n",
    "        if 'rating-title' in elt_span['class']:\n",
    "            print('rating-title')\n",
    "            print(elt_span.get_text(strip = True))\n",
    "            if 'Spectateurs' in elt_span.get_text(strip = True):\n",
    "                ratings['spectateurs'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "        #     # print(elt_span.get_text(strip = True))\n",
    "        # elif 'rating-title' in elt_span['class']:\n",
    "        #     if 'Presse' in elt_span.get_text(strip = True):\n",
    "        #         ratings['Presse'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "    print(\"ratings:\", ratings)\n",
    "\n",
    "\n",
    "driver.get(url_films)\n",
    "elts_decades = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section[3]/div[1]/div/div[3]/div[2]/ul/li')\n",
    "\n",
    "for elt_decade in elts_decades:\n",
    "    elt_a = elt_decade.find_element(By.TAG_NAME, 'a')\n",
    "\n",
    "    if elt_a.get_attribute('title')[:4] in decades_to_scrap:\n",
    "        url_decades = elt_a.get_attribute('href')\n",
    "        print(\"decade:\", url_decades)\n",
    "        driver2 = webdriver.Chrome(options = options)\n",
    "        driver2.get(url_decades)\n",
    "\n",
    "        elts_years = driver2.find_elements(By.XPATH, '/html/body/div[2]/main/section[3]/div[1]/div/div[3]/div[3]/ul/li')\n",
    "        #print(len(elts_years))\n",
    "        \n",
    "        for elt_year in elts_years[::-1]:\n",
    "            elt_a_year = elt_year.find_element(By.TAG_NAME, 'a')\n",
    "\n",
    "            url_year = elt_a_year.get_attribute('href')\n",
    "            print(\"year:\", url_year)\n",
    "\n",
    "            r = requests.get(url_year, auth=('user', 'pass'))\n",
    "            if r.status_code != 200:\n",
    "                print(\"url_site error\")\n",
    "\n",
    "            # We get the number of pages for this year\n",
    "            soup_year = BeautifulSoup(r.content, 'html.parser')\n",
    "            nb_pages = number_pages_per_year(soup_year)\n",
    "            # print('Nb pages:', nb_pages)\n",
    "\n",
    "            for i in range(nb_pages): # Need to reduce as some movies are totaly unknown with very few informations about\n",
    "                url_year_page = url_year + f'?page={i+1}'\n",
    "                # print(url_year_page)\n",
    "                r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "                if r.status_code != 200:\n",
    "                    print(\"url_site error\")\n",
    "                soup_movies = BeautifulSoup(r.content, 'html.parser')\n",
    "                elt_movies = soup_movies.find_all('li', class_='mdl')\n",
    "                #print(\"nb movies per page:\", len(elt_movies))\n",
    "                for elt_movie in elt_movies[:1]:\n",
    "                    scrap_movie(elt_movie)\n",
    "                    # break\n",
    "                break\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ou bien nous pouvons entrer les url manuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "url_decades = url_films + 'decennie-1980/'\n",
    "url_year = url_films + 'decennie-1980/annee-1980/'\n",
    "\n",
    "def getNumberOfPages(elt):\n",
    "    nb = 0\n",
    "    while elt:\n",
    "        if elt.text.isdigit():\n",
    "            nb = elt.text\n",
    "        elt = elt.find_next_sibling()\n",
    "    return int(nb)\n",
    "\n",
    "r = requests.get(url_year, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "\n",
    "# We get the number of pages for this year\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "pagination = soup.find('div', class_='pagination-item-holder')\n",
    "nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "assert nb_pages == getNumberOfPages(pagination.find('span'))\n",
    "print(nb_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape chaque page de films pour une année donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_year' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m movies \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 76\u001b[0m     url_year_page \u001b[38;5;241m=\u001b[39m \u001b[43murl_year\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?page=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n\u001b[0;32m     77\u001b[0m     r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url_year_page, auth\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url_year' is not defined"
     ]
    }
   ],
   "source": [
    "def scrapMoviePage(url_movie):\n",
    "    # get soup from movie page\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soupMovie = BeautifulSoup(r.content, 'html.parser')\n",
    "    print(\"Title:\" , soupMovie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text)\n",
    "    \n",
    "    elt = soupMovie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "\n",
    "    s1, s2, s3 = text.split('|')\n",
    "    date = s1[:-8].strip()\n",
    "    print(\"Date:\", date)\n",
    "    duration = s2.strip()\n",
    "    print(\"Duration:\", duration)\n",
    "    categories = s3.strip()\n",
    "    print(\"Categories:\", categories)\n",
    "\n",
    "    elts = soupMovie.find_all('div', class_ = \"meta-body-item meta-body-direction meta-body-oneline\")\n",
    "    #assert len(elts)\n",
    "    authors = [elts[0].text.strip()[2:].strip()]\n",
    "\n",
    "    if len(elts) > 1:\n",
    "        elts_span = elts[1].find_all('span')\n",
    "        for elt in elts_span:\n",
    "            if 'light' in elt['class']:\n",
    "                continue\n",
    "            if elt.get_text(strip=True) not in authors:\n",
    "                authors.append(elt.get_text(strip=True))\n",
    "    authors = ', '.join(authors)\n",
    "    print(\"Authors:\", authors)\n",
    "\n",
    "    elt = soupMovie.find('div', class_ = \"meta-body-item meta-body-actor\")\n",
    "    actors = elt.get_text(strip=True)[4:]\n",
    "    print(\"Actors:\", actors)\n",
    "\n",
    "    elt = soupMovie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    elt2 = elt.find('div', class_ = \"content-txt\")\n",
    "    elt3 = elt.find('p', class_ = 'bo-p')\n",
    "    summary = elt3.text.strip()\n",
    "    print(\"Summary:\", summary)\n",
    "\n",
    "    # driver to the movie page\n",
    "    driver.get(url_movie)\n",
    "    elts_rating = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "    print(len(elts))\n",
    "    for elt in elts:\n",
    "        #print(elt.text)\n",
    "        elt_a = elt.find_element(By.TAG_NAME, 'a')\n",
    "        print(elt_a.get_attribute('href'))\n",
    "\n",
    "    return\n",
    "    # beautiful soup version\n",
    "    elts_rating = soupMovie.find_all('div', class_ = 'rating-item')\n",
    "    #print(len(elts_rating))\n",
    "    ratings = {}\n",
    "\n",
    "    for elt_rating in elts_rating:\n",
    "        print(elt_rating.prettify())\n",
    "        elt_temp = elt_rating.find('div', class_='rating-item-content')\n",
    "        elt_span = elt_temp.find('span')\n",
    "        print(\"span class\", elt_span['class'])\n",
    "        if 'rating-title' in elt_span['class']:\n",
    "            print('rating-title')\n",
    "            print(elt_span.get_text(strip = True))\n",
    "            if 'Spectateurs' in elt_span.get_text(strip = True):\n",
    "                ratings['spectateurs'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "        #     # print(elt_span.get_text(strip = True))\n",
    "        # elif 'rating-title' in elt_span['class']:\n",
    "        #     if 'Presse' in elt_span.get_text(strip = True):\n",
    "        #         ratings['Presse'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "    print(\"ratings:\", ratings)\n",
    "\n",
    "movies = []\n",
    "\n",
    "for i in range(1, 3):\n",
    "    url_year_page = url_year + '?page=' + str(i)\n",
    "    r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "    if r.status_code != 200:\n",
    "        print(\"url_site error\")\n",
    "    soupMovies = BeautifulSoup(r.content, 'html.parser')\n",
    "    eltMovies = soupMovies.find_all('li', class_='mdl')\n",
    "\n",
    "    for eltMovie in eltMovies:\n",
    "        url_movie = url_site + eltMovie.h2.a.get('href')\n",
    "        scrapMoviePage(url_movie)\n",
    "    #    print()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape les pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltCountries = eltYears.find_next_sibling()\n",
    "print(eltCountries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Spectateurs\n",
      "3,0\n",
      "18413 notes, 225 critiques\n",
      "https://www.allocine.fr/film/fichefilm-4403/critiques/spectateurs/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "driver.get(\"https://www.allocine.fr/film/fichefilm_gen_cfilm=4403.html\")\n",
    "\n",
    "# elem = driver.find_element(By.NAME, \"q\")\n",
    "# elem.clear()\n",
    "# elem.send_keys(\"pycon\")\n",
    "# elem.send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "# elts = driver.find_elements(By.CLASS_NAME, 'xXx rating-title')\n",
    "elts = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "print(len(elts))\n",
    "for elt in elts[:1]:\n",
    "    print(elt.text)\n",
    "    elt_a = elt.find_element(By.TAG_NAME, 'a')\n",
    "    print(elt_a.get_attribute('href'))\n",
    "\n",
    "#elts_rating = elt.find_elements(By.XPATH, '/html/body/div[2]/main/section/div/div[3]/div[2]/div')\n",
    "# for elt in elts_rating:\n",
    "#     elt_temp = elt.find_element(By.CLASS_NAME, 'rating-item-content')\n",
    "#     elt_a = driver.find_element(By.TAG_NAME, 'a')\n",
    "    # elt_a = elt_temp.find_element(By.CLASS_NAME, 'xXx')\n",
    "    # elt_a = elt_temp.find_element(By.CLASS_NAME, 'rating-title')\n",
    "    # elt_a = elt.find_element(By.CLASS_NAME, 'xXx rating-title')\n",
    "    # print(elt_a.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
