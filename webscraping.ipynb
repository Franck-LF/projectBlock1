{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Webscraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Présentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Webscraping du site [www.allocine.fr](https://www.allocine.fr/films/) avec Beautiful Soup et Selenium.\n",
    "\n",
    "Nous vérifions tout d'abord le fichier \"robots.txt\" pour voir si nous sommes autorisés à scraper le film.\n",
    "\n",
    "![robots.txt](images/robots.txt.png)\n",
    "\n",
    "Il n'y a pas de limitation pour notre tâche puisque nous travaillons sous l'url : <code>https://www.allocine.fr/film/</code><br>\n",
    "\n",
    "Ensuite nous allons sur le site allocine, catégorie **films** et nous allons scraper les informations à partir des menus déroulants sur la gauche (les catégories de films, les pays et ensuite nous scraperons les films par année).\n",
    "\n",
    "![filtres](images/filtresSMALL.png)\n",
    "### Sources :\n",
    "**Beautiful Soup** :\n",
    "[beautiful-soup-4](https://beautiful-soup-4.readthedocs.io/en/latest/)<br>\n",
    "[beautiful-soup-4.readthedocs.io](https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree)<br>\n",
    "\n",
    "**Selenium** :<br>\n",
    "[selenium-python.readthedocs.io](https://selenium-python.readthedocs.io/locating-elements.html)<br>\n",
    "[selenium.dev/documentation](https://www.selenium.dev/documentation/webdriver/elements/information/)<br>\n",
    "[selenium.dev/documentation/finders/](https://www.selenium.dev/documentation/webdriver/elements/finders/)<br>\n",
    "[geeksforgeeks.org/get_property-selenium/](https://www.geeksforgeeks.org/get_property-element-method-selenium-python/)<br>\n",
    "\n",
    "Les liens sont sûrement générés aléatoirement dynamiquement, on peut utiliser XPath avec selenium<br>\n",
    "ou bien avec lxml ??<br>\n",
    "\n",
    "Sur ce lien https://medium.com/swlh/web-scraping-using-selenium-and-beautifulsoup-adfc8810240a Selenium est utilisé pour faire le scraping des urls puis ensuite beautiful soup est utilisé pour faire le scraping des pages de chaque urls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import math\n",
    "import copy\n",
    "import httpx\n",
    "import requests\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# One way to set the driver options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "def _options():\n",
    "    ''' Another way to set the options '''\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--test-type')\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--incognito')\n",
    "    options.add_argument('--disable-gpu') if os.name == 'nt' else None # Windows workaround\n",
    "    options.add_argument('--verbose')\n",
    "    return options\n",
    "\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "url_site  = 'https://www.allocine.fr/'\n",
    "url_films = 'https://www.allocine.fr/films/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On scrape la liste des genres de film**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "Nb categories : 37\n"
     ]
    }
   ],
   "source": [
    "# Scrap all categories\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(type(soup))\n",
    "\n",
    "categories = []\n",
    "elt_categories = soup.find('div', class_='filter-entity-section')\n",
    "for elt in elt_categories.find_all('li'):\n",
    "    #print(elt.prettify())\n",
    "    categories.append(elt.a.text)\n",
    "\n",
    "print(\"Nb categories :\", len(categories))\n",
    "df_categories = pd.Series(categories)\n",
    "\n",
    "dict_n_cat = {k:v for k, v in enumerate(categories)}\n",
    "dict_cat_n = {v:k for k, v in dict_n_cat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On scrape la liste des pays d'origine des films**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'U.S.A.', 'Afrique du Sud', 'Albanie', 'Algérie', 'Allemagne', \"Allemagne de l'Est\", \"Allemagne de l'Ouest\", 'Arabie Saoudite', 'Argentine', 'Arménie', 'Australie', 'Autriche', 'Belgique', 'Bengladesh', 'Bolivie', 'Bosnie-Herzégovine', 'Brésil', 'Bulgarie', 'Burkina Faso', 'Cambodge', 'Cameroun', 'Canada', 'Chili', 'Chine', 'Chypre', 'Colombie', 'Corée', 'Corée du Sud', 'Croatie', 'Cuba', \"Côte-d'Ivoire\", 'Danemark', 'Egypte', 'Emirats Arabes Unis', 'Espagne', 'Estonie', 'Finlande', 'Grande-Bretagne', 'Grèce', 'Géorgie', 'Hong-Kong', 'Hongrie', 'Inde', 'Indonésie', 'Irak', 'Iran', 'Irlande', 'Islande', 'Israël', 'Italie', 'Japon', 'Jordanie', 'kazakhstan', 'Kenya', 'Kosovo', 'Lettonie', 'Liban', 'Lituanie', 'Luxembourg', 'Macédoine', 'Malaisie', 'Maroc', 'Mexique', 'Monténégro', 'Nigéria', 'Norvège', 'Nouvelle-Zélande', 'Pakistan', 'Palestine', 'Pays-Bas', 'Philippines', 'Pologne', 'Portugal', 'Pérou', 'Qatar', 'Roumanie', 'Russie', 'République dominicaine', 'République tchèque', 'Serbie', 'Singapour', 'Slovaquie', 'Slovénie', 'Sri Lanka', 'Suisse', 'Suède', 'Syrie', 'Sénégal', 'Taïwan', 'Tchécoslovaquie', 'Thaïlande', 'Tunisie', 'Turquie', 'Ukraine', 'URSS', 'Uruguay', 'Vietnam', 'Vénézuela', 'Yougoslavie']\n",
      "Nb pays : 100\n"
     ]
    }
   ],
   "source": [
    "# Scrap all countries\n",
    "elt_countries = elt_categories.find_next_sibling().find_next_sibling()\n",
    "elts_items = elt_countries.find_all('li', class_ = 'filter-entity-item')\n",
    "\n",
    "countries = []\n",
    "for elt_item in elts_items:\n",
    "    countries.append(elt_item.find('span').text.strip())\n",
    "\n",
    "print(countries)\n",
    "\n",
    "print(\"Nb pays :\", len(countries))\n",
    "df_countries = pd.Series(countries)\n",
    "\n",
    "dict_n_countries = {k:v for k, v in enumerate(countries)}\n",
    "dict_countries_n = {v:k for v, k in dict_n_countries.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On scrape les liens redirigeants vers les pages de films par année**\n",
    "\n",
    "**Nous utilisons Selenium**<br>\n",
    "Lors de l'utilisation de Beautiful Soup, certains éléments sont **décorés**, certains liens sont **invisibles**, on ne peut pas directement les scraper.<br>\n",
    "Le contournement trouvé est d'utiliser Selenium qui permet entre autre :\n",
    "- d'utiliser les XPath,\n",
    "- de récupérer tous les élements et non-décorés.\n",
    "\n",
    "On se donne une liste d'années, par exemple [1980, ... 2000]\n",
    "\n",
    "[1980 - 1989] puis [1990 - 1999] ... jusqu'à [2000 - 2009].<br>\n",
    "(cela fait plus de 40000 films et XXX reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Liens visualisés dans l'inspecteur html de Chrome**\n",
    "\n",
    "![links_decades_inspector](images/link_decades_inspector.png)\n",
    "\n",
    "\n",
    "**Liens visualisés avec Beautiful Soup**\n",
    "\n",
    "![link](images/link_decades_bs4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the limit of Beautiful Soup\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# print(soup.prettify())\n",
    "\n",
    "elt_decades = elt_categories.find_next_sibling()\n",
    "print(elt_decades.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:04<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1980   ----  link https://www.allocine.fr/films/decennie-1980/annee-1980/\n"
     ]
    }
   ],
   "source": [
    "# Scrap the links of the years\n",
    "\n",
    "# Input: list of years to scrap\n",
    "lst_years_to_scrap = list(range(1980, 2009))\n",
    "lst_years_to_scrap = [1980]\n",
    "\n",
    "lst_decades_to_scrap = list(set([10 * (year // 10) for year in lst_years_to_scrap]))\n",
    "lst_years_to_scrap = [str(year) for year in lst_years_to_scrap]\n",
    "lst_decades_to_scrap = [str(decade) for decade in lst_decades_to_scrap]\n",
    "\n",
    "driver = webdriver.Chrome(options = _options())\n",
    "driver.get(url_films)\n",
    "elts_decades = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section[4]/div[1]/div/div[3]/div[2]/ul/li')\n",
    "\n",
    "dict_year_link = {}\n",
    "for elt_decade in tqdm(elts_decades):\n",
    "    elt_a = elt_decade.find_element(By.TAG_NAME, 'a')\n",
    "    if not(elt_a.get_attribute('title')[:4] in lst_decades_to_scrap):\n",
    "        continue\n",
    "\n",
    "    driver2 = webdriver.Chrome(options = options)\n",
    "    url_decade = elt_a.get_attribute('href').strip()\n",
    "\n",
    "    driver2.get(url_decade)\n",
    "    elts_years = driver2.find_elements(By.XPATH, '/html/body/div[2]/main/section[4]/div[1]/div/div[3]/div[3]/ul/li')\n",
    "\n",
    "    for elt_year in elts_years:\n",
    "        year = elt_year.find_element(By.TAG_NAME, 'a').get_attribute('title').strip()\n",
    "        if year in lst_years_to_scrap:\n",
    "            link = elt_year.find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n",
    "            dict_year_link[year] = link\n",
    "    driver2.close()\n",
    "\n",
    "for year, url_year in dict_year_link.items():\n",
    "    print(\"year\", year, '  ----  link', url_year)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **On scrape les films à partir des liens vers les années**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pour scraper les directeurs et acteurs nous allons sur la page **casting** du film puis nous scrapons les acteurs principaux représentés dans la mosaïque, ensuite nous scrapons la liste des acteurs secondaires.\n",
    "\n",
    "![all_actors](images/scraping_all_actors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe comment récupérere l'url de la page des films similaires en fonction de la page du film :<br>\n",
    "https://www.allocine.fr/film/fichefilm_gen_cfilm=180.html<br>\n",
    "https://www.allocine.fr/film/fichefilm-180/similaire/<br>\n",
    "\n",
    "Cela suit toujours le même modèle, nous allons pouvoir automatiser cela sans scraper les urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping year: 1980\n",
      "*** Page  1  ***\n",
      "Title: Elephant Man\n",
      "Title: Cannibal Holocaust\n",
      "Title: Shining\n",
      "Title: La Boum\n",
      "Title: Beau-Père\n",
      "Title: Quelque part dans le temps\n",
      "Title: Y a-t-il un pilote dans l'avion ?\n",
      "Title: Star Wars : Episode V - L'Empire contre-attaque\n"
     ]
    }
   ],
   "source": [
    "def number_pages_per_year(soup_year):\n",
    "    ''' Return the number of pages for one year'''\n",
    "    pagination = soup_year.find('div', class_='pagination-item-holder')\n",
    "    nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "    return int(nb_pages)\n",
    "\n",
    "def delete_thumbnails():\n",
    "    '''Delete all files in thumbnail directory'''\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        files = os.listdir(folder_name)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_name, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        print(\"All files deleted successfully.\")\n",
    "    except OSError:\n",
    "        print(\"Error occurred while deleting files.\")\n",
    "\n",
    "def get_title(soup_movie):\n",
    "    ''' Return the title of the movie '''\n",
    "    return soup_movie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text\n",
    "\n",
    "def get_date_duration_categories(soup_movie):\n",
    "    ''' Return date, duration and categories (as string) of the movie '''\n",
    "    elt = soup_movie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "    # print(text)\n",
    "    date, duration, categories = '', '', ''\n",
    "    if text.count('|') == 1:\n",
    "        s1, s2 = text.split('|')\n",
    "        categories = s2.strip()\n",
    "    elif text.count('|') == 2:\n",
    "        s1, s2, s3 = text.split('|')\n",
    "        date = s1[:-8].strip()\n",
    "        duration = s2.strip()\n",
    "        categories = s3.strip()\n",
    "    return date, duration, categories\n",
    "\n",
    "def get_country(soup_movie):\n",
    "    ''' Return country of the movie '''\n",
    "    elts_section_title = soup_movie.find_all('div', class_ = 'section-title')\n",
    "    for elt_section_title in elts_section_title:\n",
    "        elt_h2 = elt_section_title.find('h2')\n",
    "\n",
    "        if elt_h2 and 'Infos techniques' in elt_h2.text.strip():\n",
    "            elt_country = elt_section_title.find_next_sibling()\n",
    "            assert \"Nationalité\" in elt_country.find(\"span\", class_ = \"what light\").text.strip()\n",
    "            elt_span_that = elt_country.find(\"span\", class_ = \"that\")\n",
    "            elts_span_country = elt_span_that.find_all(\"span\")\n",
    "            lst_countries = []\n",
    "            for elt_span_country in elts_span_country:\n",
    "                lst_countries.append(elt_span_country.text.strip())\n",
    "            return ','.join(lst_countries)\n",
    "    return None\n",
    "\n",
    "def get_directors(soup_casting):\n",
    "    ''' Return list of directors '''\n",
    "    elt_director_section = soup_casting.find('section', class_='section casting-director')\n",
    "    lst_directors = []\n",
    "    if elt_director_section:\n",
    "        elt_temp = elt_director_section.find_next()\n",
    "        elts_directors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "        lst_directors = [elt_director.find('a').text for elt_director in elts_directors]\n",
    "    return lst_directors\n",
    "\n",
    "def get_actors(soup_casting):\n",
    "    ''' Return list of actors (maximum 30) '''\n",
    "    elt_actor_section = soup_casting.find('section', class_ = 'section casting-actor')\n",
    "    if not(elt_actor_section):\n",
    "        return []\n",
    "    elt_temp = elt_actor_section.find_next()\n",
    "    # scrap main actors (maximum eight actors in the mosaic, see image above)\n",
    "    elts_actors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_actors = [elt_actor.find('figure').find('span')['title'] for elt_actor in elts_actors]\n",
    "    elts_actors = elt_actor_section.find_all('div', class_ = 'md-table-row')\n",
    "    # scrap list of actors below the mosaic (we scrap maximum of (8 + 22) 30 actors in total)\n",
    "    lst_actors.extend([elt_actor.find('a').text for elt_actor in elts_actors[:22] if elt_actor.find('a')])\n",
    "    return lst_actors\n",
    "\n",
    "def get_composers(soup_casting):\n",
    "    ''' Scrap the name(s) of the music composer(s) '''\n",
    "    elts_sections = soup_casting.find_all(\"div\", class_ = \"section casting-list-gql\")\n",
    "    for elt_section in elts_sections:\n",
    "        elt_title = elt_section.find('div', class_ = 'titlebar section-title').find('h2')\n",
    "        if 'Soundtrack' in elt_title.text:\n",
    "            lst_composers = []\n",
    "            elts_composers = elt_section.find_all('div', class_ = 'md-table-row')\n",
    "            for elt_composer in elts_composers:\n",
    "                elts_span = elt_composer.find_all('span')\n",
    "                if len(elts_span) > 1 and 'Compositeur' in elts_span[1].text.strip():\n",
    "                    lst_composers.append(elts_span[0].text.strip())\n",
    "            return lst_composers\n",
    "\n",
    "def get_summary(soup_movie):\n",
    "    elt_synopsis = soup_movie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    if elt_synopsis:\n",
    "        elt_content = elt_synopsis.find('p', class_ = 'bo-p')\n",
    "        if elt_content:\n",
    "            return elt_content.text.strip()\n",
    "    return ''\n",
    "\n",
    "def get_thumbnail(soup_movie):\n",
    "    elt = soup_movie.find('figure', class_ = 'thumbnail')\n",
    "    return elt.span.img['src']\n",
    "\n",
    "def save_thumbnail(title, url_thumbnail):\n",
    "    '''Save the thumbnail as image file in directory \"thumbnails\"'''\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        title2 = title.replace('-', '')\n",
    "        image_name = f\"thumbnail-{title2}.jpg\"\n",
    "        file = open(folder_name + image_name, \"wb\")\n",
    "        image = httpx.get(url_thumbnail)\n",
    "        file.write(image.content)\n",
    "        # Display thumbnail in Jupyter / console\n",
    "        img = Image.open(io.BytesIO(image.content))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        # To change resolution: https://www.geeksforgeeks.org/change-image-resolution-using-pillow-in-python/\n",
    "    except IOError:\n",
    "        print(\"Cannot read the file\")\n",
    "    finally:\n",
    "        file.close()\n",
    "\n",
    "def get_similar_movies(url_similar_movies):\n",
    "    ''' return list of similar movies '''\n",
    "\n",
    "    lst_similar_movies = []\n",
    "    # print('url_similar_movies:', url_similar_movies)\n",
    "\n",
    "    # get the 'similar movies page' soup\n",
    "    r = requests.get(url_similar_movies, auth=('user', 'pass'))\n",
    "    soup_similar_movie = BeautifulSoup(r.content, 'html.parser')\n",
    "    if r.status_code != 200:\n",
    "        return lst_similar_movies\n",
    "    \n",
    "    elts_section = soup_similar_movie.find_all('ul', class_ = \"section\")\n",
    "    if elts_section:\n",
    "        elts_similar_movies = elts_section[0].find_all('li', class_ = 'mdl')\n",
    "        if elts_similar_movies:\n",
    "            for elt_similar_movie in elts_similar_movies:\n",
    "                elt_title = elt_similar_movie.find('h2', class_ = 'meta-title')\n",
    "                lst_similar_movies.append(elt_title.find('a').text.strip())\n",
    "\n",
    "    return lst_similar_movies\n",
    "\n",
    "def scrap_movie(elt_movie, use_Selenium):\n",
    "    ''' scrap all movie informations '''\n",
    "    \n",
    "    # get the movie soup\n",
    "    url_movie = url_site + elt_movie.h2.a.get('href')[1:]\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soup_movie = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    # ------------- #\n",
    "    #     Title     #\n",
    "    # ------------- #\n",
    "    title = get_title(soup_movie)\n",
    "\n",
    "    # ------------ #\n",
    "    #    Ratings   #\n",
    "    # ------------ #\n",
    "    star_rating, nb_notes, nb_critics = get_ratings(soup_movie, use_Selenium)\n",
    "    # print(\"star rating:\", star_rating, nb_notes, nb_critics)\n",
    "    nb_critics = convert_to_integer(nb_critics)\n",
    "    if nb_critics < 20:\n",
    "        print(\"Not enough critics, Do not scrape the movie:\" , title)\n",
    "        # print(url_movie)\n",
    "        return 'Crit', None\n",
    "\n",
    "    # --------------------------------- #\n",
    "    #   Date, duration and categories   #\n",
    "    # --------------------------------- #\n",
    "    date, duration, categories = get_date_duration_categories(soup_movie)\n",
    "    # print(\"Date:\", date)\n",
    "    # print(\"Duration:\", duration)\n",
    "    # print(\"Categories:\", categories)\n",
    "    \n",
    "    if 'Documentaire' in categories or 'Erotique' in categories or categories.strip() == 'Divers':\n",
    "        print('We do not scrape those category film:', title)\n",
    "        # print(url_movie)\n",
    "        return 'Cat', None\n",
    "    \n",
    "    print('Title:', title)\n",
    "    # print(url_movie)\n",
    "\n",
    "    # ----------------- #\n",
    "    #      Countries    #\n",
    "    # ----------------- #\n",
    "    lst_countries = get_country(soup_movie)\n",
    "\n",
    "    # ------------------------------------------------- #\n",
    "    #   Directors list / Actors list / Composers list   #\n",
    "    # ------------------------------------------------- #\n",
    "    lst_directors, lst_actors, lst_composers = [], [], []\n",
    "    is_casting_section = False\n",
    "    elts_end_section = soup_movie.find_all('a', class_ = 'end-section-link')\n",
    "\n",
    "    if elts_end_section:\n",
    "        for elt_end_section in elts_end_section:\n",
    "\n",
    "            if 'Casting' in elt_end_section['title']:\n",
    "                # If there is a link to the casting section\n",
    "                is_casting_section = True\n",
    "                link_casting = elt_end_section['href']\n",
    "                r = requests.get(url_site + link_casting, auth=('user', 'pass'))\n",
    "                soup_casting = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "                # Get directors' list\n",
    "                lst_directors = get_directors(soup_casting)\n",
    "                # Get actors' list\n",
    "                lst_actors = get_actors(soup_casting)\n",
    "                # Composers' list\n",
    "                lst_composers = get_composers(soup_casting)\n",
    "                break \n",
    "\n",
    "    if not(is_casting_section):\n",
    "        # No casting section \n",
    "        # for example animation movies does not have a casting section\n",
    "        # some movies neither: https://www.allocine.fr/film/fichefilm_gen_cfilm=27635.html\n",
    "\n",
    "        # Get directors' list\n",
    "        elt_director = soup_movie.find('div', class_ = \"meta-body-item meta-body-direction\")\n",
    "        if elt_director:\n",
    "            # print(elt_director.prettify())\n",
    "            elts_span = elt_director.find_all('span')\n",
    "            assert len(elts_span) >= 2\n",
    "            lst_directors.append(elts_span[1].get_text().strip())\n",
    "\n",
    "        # Get actors' list\n",
    "        elt_actor = soup_movie.find('div', class_ = \"meta-body-item meta-body-actor\")\n",
    "        # print(elt_actor.prettify())\n",
    "        if elt_actor:\n",
    "            # print(elt_actor.prettify())\n",
    "            for elt_a in elt_actor.find_all('a'):\n",
    "                lst_actors.append(elt_a.text.strip())\n",
    "\n",
    "    # print('Directors:', lst_directors)\n",
    "    # print('Actors:', lst_actors)\n",
    "    # print('Composers:', lst_composers)\n",
    "\n",
    "    # ------------ #\n",
    "    #   Summary    #\n",
    "    # ------------ #\n",
    "    summary = get_summary(soup_movie)[:180]\n",
    "    # print(\"Summary:\", summary)\n",
    "\n",
    "    # ------------ #\n",
    "    #   Thumbnail  #\n",
    "    # ------------ #\n",
    "    url_thumbnail = get_thumbnail(soup_movie)\n",
    "    # print(\"Thumbnail:\", url_thumbnail)\n",
    "    # save_thumbnail(title, url_thumbnail)\n",
    "\n",
    "    # ------------------- #\n",
    "    #     url_reviews     #\n",
    "    # ------------------- #\n",
    "    url_reviews = url_movie.replace('_gen_cfilm=', '-')[:-5] + '/critiques/spectateurs/'\n",
    "    # print(url_reviews)\n",
    "\n",
    "    # ------------------- #\n",
    "    #    Similar Movies   #\n",
    "    # ------------------- #\n",
    "    url_similar_movies = url_movie.replace('_gen_cfilm=', '-')[:-5] + '/similaire/'\n",
    "    # soup_similar_movies\n",
    "    lst_similar_movies = get_similar_movies(url_similar_movies)\n",
    "\n",
    "    return 'OK', (title, date, duration, categories, lst_countries, star_rating, nb_notes, nb_critics, lst_directors, lst_actors, lst_composers,\\\n",
    "                  summary, url_thumbnail, url_reviews, url_similar_movies)\n",
    "\n",
    "def get_ratings(soup_movie, use_Selenium):\n",
    "    ''' Scrap the ratings of the movie.\n",
    "\n",
    "        Return:\n",
    "         - stareval:   star rating (0.5 to 5),\n",
    "         - nb_notes:   number of votes,\n",
    "         - nb_critics: number of critics (written reviews),\n",
    "\n",
    "        Args:\n",
    "         - soup_movie:   object BeautifulSoup of the movie,\n",
    "         - driver:       Selenium driver,\n",
    "         - use_Selenium: boolean to choose the method to scrape,\n",
    "                         True:  Selenium's method,      (SLOWER)\n",
    "                         False: Beautifulsoup's method. (FASTER)\n",
    "    '''\n",
    "\n",
    "    star_rating, nb_notes, nb_critics = None, None, None\n",
    "\n",
    "    if use_Selenium:\n",
    "        elts_ratings = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "\n",
    "        for elt_rating in elts_ratings:\n",
    "            elt = None\n",
    "            try:\n",
    "                elt = elt_rating.find_element(By.TAG_NAME, 'a')\n",
    "                if 'Spectateurs' in elt.text.strip():\n",
    "#                    url_reviews = elt.get_attribute('href')\n",
    "                    elt_stareval_note = elt_rating.find_element(By.CLASS_NAME, 'stareval-note')\n",
    "                    star_rating = elt_stareval_note.text.strip()\n",
    "                    elt_stareval_review = elt_rating.find_element(By.CLASS_NAME, 'stareval-review')\n",
    "                    stareval_review = elt_stareval_review.text.strip()\n",
    "                    if stareval_review.count(',') == 1:\n",
    "                        nb_notes, nb_critics = stareval_review.split(',')\n",
    "                        nb_notes = nb_notes.split()[0]\n",
    "                        nb_critics = nb_critics.split()[0]\n",
    "                    elif 'note' in stareval_review:\n",
    "                        nb_notes = stareval_review.strip()\n",
    "                    elif 'critique' in stareval_review:\n",
    "                        nb_critics = stareval_review\n",
    "                    else:\n",
    "                        assert False\n",
    "\n",
    "            except:\n",
    "                print('no tag \"a\" in elt_rating')\n",
    "\n",
    "    else: # use beautiful soup\n",
    "        elts_ratings = soup_movie.find_all('div', class_ = 'rating-item-content')\n",
    "        for elt_rating in elts_ratings:\n",
    "            if 'Spectateurs' in elt_rating.find(\"span\").text.strip():\n",
    "                elt_stareval_note = elt_rating.find(\"span\", class_ = \"stareval-note\")\n",
    "                star_rating = elt_stareval_note.text.strip()\n",
    "                elt_stareval_review = elt_rating.find(\"span\", class_ = \"stareval-review\")\n",
    "                stareval_review = elt_stareval_review.text.strip()\n",
    "                if stareval_review.count(',') == 1:\n",
    "                    nb_notes, nb_critics = stareval_review.split(',')\n",
    "                    nb_notes = nb_notes.split()[0]\n",
    "                    nb_critics = nb_critics.split()[0]\n",
    "                elif 'note' in stareval_review:\n",
    "                    nb_notes = stareval_review.strip()\n",
    "                elif 'critique' in stareval_review:\n",
    "                    nb_critics = stareval_review\n",
    "                else:\n",
    "                    assert False\n",
    "\n",
    "    return star_rating, nb_notes, nb_critics\n",
    "\n",
    "def convert_to_integer(str_nb_critics):\n",
    "    if not(str_nb_critics):\n",
    "        return 0\n",
    "    test = re.search('\\\\d+', str_nb_critics)\n",
    "    return int(test.string)\n",
    "\n",
    "\n",
    "# ---------------------------------- #\n",
    "#                                    #\n",
    "#             Main loop              #\n",
    "#                                    #\n",
    "# ---------------------------------- #\n",
    "        \n",
    "# loop on all years to scrap (through links previously scrapped)\n",
    "# then loop on all pages of the year\n",
    "# then loop on all movies on one page\n",
    "# \n",
    "\n",
    "# delete_thumbnails()\n",
    "\n",
    "use_Selenium = False\n",
    "\n",
    "# Create Selenium driver\n",
    "driver = None\n",
    "if use_Selenium:\n",
    "    driver = webdriver.Chrome(options = _options())\n",
    "\n",
    "counter_movies                          = 0\n",
    "counter_scraped_movies                  = 0\n",
    "counter_not_scraped_not_enough_reviews  = 0\n",
    "counter_not_scraped_categories          = 0\n",
    "movies = []\n",
    "\n",
    "for year, url_year in dict_year_link.items():\n",
    "    print(\"Scraping year:\", year)\n",
    "    \n",
    "    r = requests.get(url_year, auth=('user', 'pass'))\n",
    "    if r.status_code != 200:\n",
    "        print(\"url_site error\")\n",
    "\n",
    "    soup_year = BeautifulSoup(r.content, 'html.parser')\n",
    "    nb_pages = number_pages_per_year(soup_year)\n",
    "    # print(\"Nb pages :\", nb_pages)\n",
    "\n",
    "    for i in range(nb_pages): # Need to reduce as some movies are totaly unknown with very few informations about\n",
    "        url_year_page = url_year + f'?page={i+1}'\n",
    "        r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "        if r.status_code != 200:\n",
    "            print(\"url_site error\")\n",
    "\n",
    "        print(\"*** Page \", i+1, \" ***\")\n",
    "        soup_movies = BeautifulSoup(r.content, 'html.parser')\n",
    "        elt_movies = soup_movies.find_all('li', class_='mdl')\n",
    "\n",
    "        for elt_movie in elt_movies:\n",
    "            # print('---------------------------------------------------------------- ')\n",
    "            status, movie = scrap_movie(elt_movie, use_Selenium)\n",
    "            counter_movies += 1\n",
    "            \n",
    "            if status == 'OK':\n",
    "                counter_scraped_movies += 1\n",
    "                movies.append(movie)\n",
    "            else:\n",
    "                counter_not_scraped_not_enough_reviews  += int(status == 'Crit')\n",
    "                counter_not_scraped_categories          += int(status == 'Cat')\n",
    "\n",
    "if driver:\n",
    "    driver.close()\n",
    "    \n",
    "print(\"Nb movies scanned: \", counter_movies)\n",
    "print(\"Nb movies scrapped:\", counter_scraped_movies)\n",
    "print(\"Not scrapped Crit: \", counter_not_scraped_not_enough_reviews)\n",
    "print(\"Not scrapped Cat:  \", counter_not_scraped_categories)\n",
    "\n",
    "df_movies = pd.DataFrame(movies, columns = ['title', 'date', 'duration', 'categories', 'countries', 'star_rating', 'notes', 'critics',\\\n",
    "                                            'directors', 'actors', 'composers', 'summary', 'url_thumbnail', 'url_reviews', 'url_similar_movies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
