{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> webscraping\n",
    "\n",
    "### première partie du projet bloc 1\n",
    "\n",
    "- Webscraping du site [www.allocine.fr](https://www.allocine.fr/films/)\n",
    "\n",
    "![filtres](images/filtresSMALL.png)\n",
    "\n",
    "## Sources :\n",
    "[beautiful-soup-4](https://beautiful-soup-4.readthedocs.io/en/latest/)<br>\n",
    "Les liens sont sûrement générés aléatoirement dynamiquement, on peut utiliser XPath avec selenium<br>\n",
    "ou bien avec lxml ??<br>\n",
    "[beautiful-soup-4.readthedocs.io](https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree)<br>\n",
    "\n",
    "[selenium-python.readthedocs.io](https://selenium-python.readthedocs.io/locating-elements.html)<br>\n",
    "[selenium.dev/documentation](https://www.selenium.dev/documentation/webdriver/elements/information/)<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "#import httpx\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options = options)\n",
    "\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "url_site = 'https://www.allocine.fr/'\n",
    "url_films = 'https://www.allocine.fr/films/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape tous les genres de film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "# Scrap all categories\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(type(soup))\n",
    "\n",
    "categories = []\n",
    "eltCategories = soup.find('div', class_='filter-entity-section')\n",
    "for elt in eltCategories.find_all('li'):\n",
    "    categories.append(elt.a.text)\n",
    "\n",
    "df_categories = pd.Series(categories)\n",
    "\n",
    "dict_n_cat = {k:v for k, v in enumerate(categories)}\n",
    "dict_cat_n = {v:k for v, k in dict_n_cat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrapte les films par période\n",
    "[1980 - 1989] puis [1990 - 1999] ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030 - 2039\n",
      "2020 - 2029\n",
      "2010 - 2019\n",
      "2000 - 2009\n",
      "1990 - 1999\n",
      "** 1980 - 1989\n",
      "1970 - 1979\n",
      "1960 - 1969\n",
      "1950 - 1959\n",
      "1940 - 1949\n",
      "1930 - 1939\n",
      "1920 - 1929\n",
      "1910 - 1919\n",
      "1900 - 1909\n",
      "1890 - 1899\n"
     ]
    }
   ],
   "source": [
    "# Scrap url of years we want to scrap the movies\n",
    "# Not Working\n",
    "# I cannot get the url by scraping\n",
    "decades_to_scrap = ['1980 - 1989']\n",
    "eltYears = eltCategories.find_next_sibling()\n",
    "lstUrl = []\n",
    "eltCur = eltYears.find('li')\n",
    "\n",
    "while eltCur:\n",
    "    text = eltCur.span.text\n",
    "    # print(eltCur)\n",
    "    # print(eltCur.span.get('href'))\n",
    "    if text in decades_to_scrap:\n",
    "        print(\"**\", text)\n",
    "    else:\n",
    "        print(eltCur.span.text)\n",
    "    eltCur = eltCur.find_next_sibling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ou bien nous pouvons entrer les url manuellement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "url_decades = url_films + 'decennie-1980/'\n",
    "url_year = url_films + 'decennie-1980/annee-1980/'\n",
    "\n",
    "def getNumberOfPages(elt):\n",
    "    nb = 0\n",
    "    while elt:\n",
    "        if elt.text.isdigit():\n",
    "            nb = elt.text\n",
    "        elt = elt.find_next_sibling()\n",
    "    return int(nb)\n",
    "\n",
    "r = requests.get(url_year, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "\n",
    "# We get the number of pages for this year\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "pagination = soup.find('div', class_='pagination-item-holder')\n",
    "nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "assert nb_pages == getNumberOfPages(pagination.find('span'))\n",
    "print(nb_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape chaque page de films pour une année donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: La Boum\n",
      "Title: Shining\n",
      "Title: Elephant Man\n",
      "Title: Star Wars : Episode V - L'Empire contre-attaque\n",
      "Title: American Gigolo\n",
      "Title: Cannibal Holocaust\n",
      "Title: Les Sous-doués\n",
      "Title: Le Roi et l'oiseau\n",
      "Title: Y a-t-il un pilote dans l'avion ?\n",
      "Title: Raging Bull\n",
      "Title: Le Dernier métro\n",
      "Title: Pulsions\n",
      "Title: The Blues Brothers\n",
      "Title: Beau-Père\n",
      "Title: Je vais craquer\n",
      "Title: La Chasse - Cruising\n",
      "Title: Le Miroir se brisa\n",
      "Title: Le Lagon Bleu\n",
      "Title: La Porte du paradis\n",
      "Title: Popeye\n",
      "Title: Ça va cogner\n",
      "Title: Le Petit Lord Fauntleroy\n",
      "Title: Vendredi 13\n",
      "Title: Pile ou face\n",
      "Title: Gloria\n",
      "Title: Au-delà du réel\n",
      "Title: La Banquière\n",
      "Title: La Femme flic\n",
      "Title: Kagemusha, l'ombre du guerrier\n",
      "Title: Fog\n"
     ]
    }
   ],
   "source": [
    "def scrapMoviePage(url_movie):\n",
    "    # get soup from movie page\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soupMovie = BeautifulSoup(r.content, 'html.parser')\n",
    "    print(\"Title:\" , soupMovie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text)\n",
    "    \n",
    "    elt = soupMovie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "\n",
    "    s1, s2, s3 = text.split('|')\n",
    "    date = s1[:-8].strip()\n",
    "    print(\"Date:\", date)\n",
    "    duration = s2.strip()\n",
    "    print(\"Duration:\", duration)\n",
    "    categories = s3.strip()\n",
    "    print(\"Categories:\", categories)\n",
    "\n",
    "    elts = soupMovie.find_all('div', class_ = \"meta-body-item meta-body-direction meta-body-oneline\")\n",
    "    #assert len(elts)\n",
    "    authors = [elts[0].text.strip()[2:].strip()]\n",
    "\n",
    "    if len(elts) > 1:\n",
    "        elts_span = elts[1].find_all('span')\n",
    "        for elt in elts_span:\n",
    "            if 'light' in elt['class']:\n",
    "                continue\n",
    "            if elt.get_text(strip=True) not in authors:\n",
    "                authors.append(elt.get_text(strip=True))\n",
    "    authors = ', '.join(authors)\n",
    "    print(\"Authors:\", authors)\n",
    "\n",
    "    elt = soupMovie.find('div', class_ = \"meta-body-item meta-body-actor\")\n",
    "    actors = elt.get_text(strip=True)[4:]\n",
    "    print(\"Actors:\", actors)\n",
    "\n",
    "    elt = soupMovie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    elt2 = elt.find('div', class_ = \"content-txt\")\n",
    "    elt3 = elt.find('p', class_ = 'bo-p')\n",
    "    summary = elt3.text.strip()\n",
    "    print(\"Summary:\", summary)\n",
    "\n",
    "    # driver to the movie page\n",
    "    driver.get(url_movie)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return\n",
    "    # beautiful soup version\n",
    "    elts_rating = soupMovie.find_all('div', class_ = 'rating-item')\n",
    "    #print(len(elts_rating))\n",
    "    ratings = {}\n",
    "\n",
    "    for elt_rating in elts_rating:\n",
    "        print(elt_rating.prettify())\n",
    "        elt_temp = elt_rating.find('div', class_='rating-item-content')\n",
    "        elt_span = elt_temp.find('span')\n",
    "        print(\"span class\", elt_span['class'])\n",
    "        if 'rating-title' in elt_span['class']:\n",
    "            print('rating-title')\n",
    "            print(elt_span.get_text(strip = True))\n",
    "            if 'Spectateurs' in elt_span.get_text(strip = True):\n",
    "                ratings['spectateurs'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "        #     # print(elt_span.get_text(strip = True))\n",
    "        # elif 'rating-title' in elt_span['class']:\n",
    "        #     if 'Presse' in elt_span.get_text(strip = True):\n",
    "        #         ratings['Presse'] = elt_temp.find('span', class_ = 'stareval-note').text\n",
    "    print(\"ratings:\", ratings)\n",
    "\n",
    "movies = []\n",
    "\n",
    "for i in range(1, 3):\n",
    "    url_year_page = url_year + '?page=' + str(i)\n",
    "    r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "    if r.status_code != 200:\n",
    "        print(\"url_site error\")\n",
    "    soupMovies = BeautifulSoup(r.content, 'html.parser')\n",
    "    eltMovies = soupMovies.find_all('li', class_='mdl')\n",
    "\n",
    "    for eltMovie in eltMovies:\n",
    "        url_movie = url_site + eltMovie.h2.a.get('href')\n",
    "        scrapMoviePage(url_movie)\n",
    "    #    print()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On scrape les pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eltCountries = eltYears.find_next_sibling()\n",
    "print(eltCountries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Presse\n",
      "4,0\n",
      "5 critiques\n",
      "https://www.allocine.fr/film/fichefilm-863/critiques/presse/\n",
      "Spectateurs\n",
      "4,3\n",
      "58968 notes, 1750 critiques\n",
      "https://www.allocine.fr/film/fichefilm-863/critiques/spectateurs/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "driver.get(\"https://www.allocine.fr/film/fichefilm_gen_cfilm=863.html\")\n",
    "\n",
    "# elem = driver.find_element(By.NAME, \"q\")\n",
    "# elem.clear()\n",
    "# elem.send_keys(\"pycon\")\n",
    "# elem.send_keys(Keys.RETURN)\n",
    "\n",
    "\n",
    "# elts = driver.find_elements(By.CLASS_NAME, 'xXx rating-title')\n",
    "elts = driver.find_elements(By.CLASS_NAME, 'rating-item')\n",
    "print(len(elts))\n",
    "for elt in elts[:2]:\n",
    "    print(elt.text)\n",
    "    # print(elt.get_attribute('href'))\n",
    "    elt_a = elt.find_element(By.TAG_NAME, 'a')\n",
    "    print(elt_a.get_attribute('href'))\n",
    "#elts_rating = elt.find_elements(By.XPATH, '/html/body/div[2]/main/section/div/div[3]/div[2]/div')\n",
    "\n",
    "# for elt in elts_rating:\n",
    "#     elt_temp = elt.find_element(By.CLASS_NAME, 'rating-item-content')\n",
    "#     elt_a = driver.find_element(By.TAG_NAME, 'a')\n",
    "    # elt_a = elt_temp.find_element(By.CLASS_NAME, 'xXx')\n",
    "    # elt_a = elt_temp.find_element(By.CLASS_NAME, 'rating-title')\n",
    "    # elt_a = elt.find_element(By.CLASS_NAME, 'xXx rating-title')\n",
    "    # print(elt_a.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
