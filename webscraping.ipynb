{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> webscraping\n",
    "\n",
    "## Première partie du projet bloc 1\n",
    "\n",
    "- Webscraping du site [www.allocine.fr](https://www.allocine.fr/films/) avec Beautiful Soup et Selenium\n",
    "\n",
    "![filtres](images/filtresSMALL.png)\n",
    "\n",
    "## Sources :\n",
    "**Beautiful Soup** :\n",
    "[beautiful-soup-4](https://beautiful-soup-4.readthedocs.io/en/latest/)<br>\n",
    "[beautiful-soup-4.readthedocs.io](https://beautiful-soup-4.readthedocs.io/en/latest/#searching-the-tree)<br>\n",
    "\n",
    "**Selenium** :<br>\n",
    "[selenium-python.readthedocs.io](https://selenium-python.readthedocs.io/locating-elements.html)<br>\n",
    "[selenium.dev/documentation](https://www.selenium.dev/documentation/webdriver/elements/information/)<br>\n",
    "[selenium.dev/documentation/finders/](https://www.selenium.dev/documentation/webdriver/elements/finders/)<br>\n",
    "[geeksforgeeks.org/get_property-selenium/](https://www.geeksforgeeks.org/get_property-element-method-selenium-python/)<br>\n",
    "\n",
    "Les liens sont sûrement générés aléatoirement dynamiquement, on peut utiliser XPath avec selenium<br>\n",
    "ou bien avec lxml ??<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import re\n",
    "import io\n",
    "import math\n",
    "import copy\n",
    "import httpx\n",
    "import requests\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "url_site = 'https://www.allocine.fr/'\n",
    "url_films = 'https://www.allocine.fr/films/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On scrape la liste des genres de film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "Nb categories : 37\n"
     ]
    }
   ],
   "source": [
    "# Scrap all categories\n",
    "r = requests.get(url_films, auth=('user', 'pass'))\n",
    "if r.status_code != 200:\n",
    "    print(\"url_site error\")\n",
    "    \n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(type(soup))\n",
    "\n",
    "categories = []\n",
    "elt_categories = soup.find('div', class_='filter-entity-section')\n",
    "for elt in elt_categories.find_all('li'):\n",
    "    #print(elt.prettify())\n",
    "    categories.append(elt.a.text)\n",
    "\n",
    "print(\"Nb categories :\", len(categories))\n",
    "df_categories = pd.Series(categories)\n",
    "\n",
    "dict_n_cat = {k:v for k, v in enumerate(categories)}\n",
    "dict_cat_n = {v:k for v, k in dict_n_cat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On scrape la liste des pays d'origine des films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'U.S.A.', 'Afrique du Sud', 'Albanie', 'Algérie', 'Allemagne', \"Allemagne de l'Est\", \"Allemagne de l'Ouest\", 'Arabie Saoudite', 'Argentine', 'Arménie', 'Australie', 'Autriche', 'Belgique', 'Bengladesh', 'Bolivie', 'Bosnie-Herzégovine', 'Brésil', 'Bulgarie', 'Burkina Faso', 'Cambodge', 'Cameroun', 'Canada', 'Chili', 'Chine', 'Chypre', 'Colombie', 'Corée', 'Corée du Sud', 'Croatie', 'Cuba', \"Côte-d'Ivoire\", 'Danemark', 'Egypte', 'Emirats Arabes Unis', 'Espagne', 'Estonie', 'Finlande', 'Grande-Bretagne', 'Grèce', 'Géorgie', 'Hong-Kong', 'Hongrie', 'Inde', 'Indonésie', 'Irak', 'Iran', 'Irlande', 'Islande', 'Israël', 'Italie', 'Japon', 'Jordanie', 'kazakhstan', 'Kenya', 'Kosovo', 'Lettonie', 'Liban', 'Lituanie', 'Luxembourg', 'Macédoine', 'Malaisie', 'Maroc', 'Mexique', 'Monténégro', 'Nigéria', 'Norvège', 'Nouvelle-Zélande', 'Pakistan', 'Palestine', 'Pays-Bas', 'Philippines', 'Pologne', 'Portugal', 'Pérou', 'Qatar', 'Roumanie', 'Russie', 'République dominicaine', 'République tchèque', 'Serbie', 'Singapour', 'Slovaquie', 'Slovénie', 'Sri Lanka', 'Suisse', 'Suède', 'Syrie', 'Sénégal', 'Taïwan', 'Tchécoslovaquie', 'Thaïlande', 'Tunisie', 'Turquie', 'Ukraine', 'URSS', 'Uruguay', 'Vietnam', 'Vénézuela', 'Yougoslavie']\n",
      "Nb pays : 100\n"
     ]
    }
   ],
   "source": [
    "# Scrap all countries\n",
    "elt_countries = elt_categories.find_next_sibling().find_next_sibling()\n",
    "elts_items = elt_countries.find_all('li', class_ = 'filter-entity-item')\n",
    "\n",
    "countries = []\n",
    "for elt_item in elts_items:\n",
    "    countries.append(elt_item.find('span').text.strip())\n",
    "\n",
    "print(countries)\n",
    "\n",
    "print(\"Nb pays :\", len(countries))\n",
    "df_countries = pd.Series(countries)\n",
    "\n",
    "dict_n_countries = {k:v for k, v in enumerate(countries)}\n",
    "dict_countries_n = {v:k for v, k in dict_n_countries.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On scrape les liens redirigeants vers les pages de films par année\n",
    "\n",
    "**Nous utilisons Selenium**\n",
    "Certains liens sont **décorés** et ne peuvent pas être directement scrapés avec **Beautiful Soup** nous utiliser Selenium qui permet entre autre :\n",
    "- d'utiliser les XPath (contrairement à Beautiful Soup)\n",
    "- de récupérer certains élements **décorés** par exemple des urls\n",
    "\n",
    "On se donne une liste d'années, par exemple [1980, ... 2000]\n",
    "\n",
    "[1980 - 1989] puis [1990 - 1999] ... jusqu'à [2000 - 2009].<br>\n",
    "(cela fait plus de 40000 films et XXX reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 1980   ----  link https://www.allocine.fr/films/decennie-1980/annee-1980/\n"
     ]
    }
   ],
   "source": [
    "# Scrap the links of the years\n",
    "\n",
    "# Input: list of years to scrap\n",
    "lst_years_to_scrap = list(range(1980, 2009))\n",
    "lst_years_to_scrap = [1980]\n",
    "\n",
    "lst_decades_to_scrap = list(set([10 * (year // 10) for year in lst_years_to_scrap]))\n",
    "lst_years_to_scrap = [str(year) for year in lst_years_to_scrap]\n",
    "lst_decades_to_scrap = [str(decade) for decade in lst_decades_to_scrap]\n",
    "\n",
    "driver = webdriver.Chrome(options = options)\n",
    "driver.get(url_films)\n",
    "elts_decades = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section[3]/div[1]/div/div[3]/div[2]/ul/li')\n",
    "\n",
    "dict_year_link = {}\n",
    "for elt_decade in tqdm(elts_decades):\n",
    "    elt_a = elt_decade.find_element(By.TAG_NAME, 'a')\n",
    "    if not(elt_a.get_attribute('title')[:4] in lst_decades_to_scrap):\n",
    "        continue\n",
    "\n",
    "    driver2 = webdriver.Chrome(options = options)\n",
    "    url_decade = elt_a.get_attribute('href').strip()\n",
    "    driver2.get(url_decade)\n",
    "    elts_years = driver2.find_elements(By.XPATH, '/html/body/div[2]/main/section[3]/div[1]/div/div[3]/div[3]/ul/li')\n",
    "\n",
    "    for elt_year in elts_years:\n",
    "        year = elt_year.find_element(By.TAG_NAME, 'a').get_attribute('title').strip()\n",
    "        if year in lst_years_to_scrap:\n",
    "            link = elt_year.find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n",
    "            dict_year_link[year] = link\n",
    "    driver2.close()\n",
    "\n",
    "for year, url_year in dict_year_link.items():\n",
    "    print(\"year\", year, '  ----  link', url_year)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On scrape les films à partir des liens vers les années\n",
    "\n",
    "Pour scraper les directeurs et acteurs nous allons sur la page **casting** du film puis nous scrapons les acteurs principaux représentés dans la mosaïque, ensuite nous scrapons la liste des acteurs secondaires.\n",
    "\n",
    "![all_actors](images/scraping_all_actors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:,\n",
      "Scraping year: 1980 https://www.allocine.fr/films/decennie-1980/annee-1980/\n",
      " ---------------------------------------------------------------- \n",
      "https://www.allocine.fr/film/fichefilm_gen_cfilm=180.html\n",
      "https://www.allocine.fr/film/fichefilm_gen_cfilm=180.html\n",
      "Title: Elephant Man\n",
      "Date: 8 avril 1981\n",
      "Duration: 2h 05min\n",
      "Categories: Biopic,Drame\n",
      "Directors: ['David Lynch']\n",
      "Actors: ['Anthony Hopkins', 'John Hurt', 'Anne Bancroft', 'John Gielgud', 'Wendy Hiller', 'Freddie Jones', 'Hannah Gordon', 'Michael Elphick', 'Lesley Dunlop', 'Kenny Baker', 'John Standing', 'Dexter Fletcher', 'Phoebe Nicholls', 'Patsy Smart', 'Frederick Treves', 'Roy Evans', 'Tony London']\n",
      "Summary: Londres, 1884. Le chirurgien Frederick Treves découvre un homme complètement défiguré et difforme, devenu une attraction de foire. John Merrick, \" le monstre \", doit son nom de Ele\n",
      "Thumbnail: https://fr.web.img4.acsta.net/c_310_420/pictures/20/02/21/16/48/4302324.jpg\n",
      "https://www.allocine.fr/film/fichefilm_gen_cfilm=180.html\n",
      "nb elt_ratings: 3\n",
      "4,4 29180 notes, 682 critiques\n"
     ]
    }
   ],
   "source": [
    "def number_pages_per_year(soup_year):\n",
    "    ''' Return the number of pages for one year'''\n",
    "    pagination = soup_year.find('div', class_='pagination-item-holder')\n",
    "    nb_pages = int(pagination.find_all('span')[-1].text)\n",
    "    return int(nb_pages)\n",
    "\n",
    "def delete_thumbnails():\n",
    "    '''Delete all files in thumbnail directory'''\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        files = os.listdir(folder_name)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(folder_name, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        print(\"All files deleted successfully.\")\n",
    "    except OSError:\n",
    "        print(\"Error occurred while deleting files.\")\n",
    "\n",
    "def get_title(soup_movie):\n",
    "    return soup_movie.find('div', class_ = \"titlebar-title titlebar-title-xl\").text\n",
    "\n",
    "def get_date_duration_categories(soup_movie):\n",
    "    elt = soup_movie.find('div', class_=\"meta-body-item meta-body-info\")\n",
    "    text = elt.get_text(strip=True)\n",
    "    s1, s2, s3 = text.split('|')\n",
    "    date = s1[:-8].strip()\n",
    "    duration = s2.strip()\n",
    "    categories = s3.strip()\n",
    "    return date, duration, categories\n",
    "\n",
    "def get_directors(soup_casting):\n",
    "    elt_director_section = soup_casting.find('section', class_='section casting-director')\n",
    "    elt_temp = elt_director_section.find_next()\n",
    "    # print(elt_temp.prettify())\n",
    "    elts_directors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_directors = [elt_director.find('a').text for elt_director in elts_directors]\n",
    "    return lst_directors\n",
    "\n",
    "def get_actors(soup_casting):\n",
    "    elt_actor_section = soup_casting.find('section', class_ = 'section casting-actor')\n",
    "    if not(elt_actor_section):\n",
    "        return []\n",
    "    elt_temp = elt_actor_section.find_next()\n",
    "    #print(elt_temp.prettify())\n",
    "    # scrap main actors (maximum eight actors in the mosaic, see image above)\n",
    "    elts_actors = elt_temp.find_next_sibling().find_all('div', class_ = 'card person-card person-card-col')\n",
    "    lst_actors = [elt_actor.find('figure').find('span')['title'] for elt_actor in elts_actors]\n",
    "    elts_actors = elt_actor_section.find_all('div', class_ = 'md-table-row')\n",
    "    # scrap list of actors below the mosaic (we scrap maximum of (8 + 22) 30 actors in total)\n",
    "    lst_actors.extend([elt_actor.find('a').text for elt_actor in elts_actors[:22] if elt_actor.find('a')])\n",
    "    return lst_actors\n",
    "\n",
    "def get_composer(soup_casting):\n",
    "    ''' Scrap the name.s of the music composer.s '''\n",
    "    pass\n",
    "\n",
    "def get_summary(soup_movie):\n",
    "    elt = soup_movie.find('section', class_ = \"section ovw ovw-synopsis\")\n",
    "    return elt.find('p', class_ = 'bo-p').text.strip()\n",
    "\n",
    "def get_thumbnail(soup_movie):\n",
    "    elt = soup_movie.find('figure', class_ = 'thumbnail')\n",
    "    return elt.span.img['src']\n",
    "\n",
    "def save_thumbnail(title, url_thumbnail):\n",
    "    '''Save the thumbnail as image file in directory \"thumbnails\"'''\n",
    "    try:\n",
    "        folder_name = os.getcwd() + '\\\\thumbnails\\\\'\n",
    "        title2 = title.replace('-', '')\n",
    "        image_name = f\"thumbnail-{title2}.jpg\"\n",
    "        file = open(folder_name + image_name, \"wb\")\n",
    "        image = httpx.get(url_thumbnail)\n",
    "        file.write(image.content)\n",
    "        # Display thumbnail in Jupyter / console\n",
    "        img = Image.open(io.BytesIO(image.content))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        # To change resolution: https://www.geeksforgeeks.org/change-image-resolution-using-pillow-in-python/\n",
    "    except IOError:\n",
    "        print(\"Cannot read the file\")\n",
    "    finally:\n",
    "        file.close()\n",
    "\n",
    "def get_ratings(soup_movie, driver):\n",
    "    ''' Scrap the ratings of the movie:\n",
    "\n",
    "        - star rating (0.5 to 5 with step 0.5),\n",
    "        - number of votes,\n",
    "        - number of critics (written reviews). \n",
    "    '''\n",
    "    \n",
    "    test = driver.find_element(By.CLASS_NAME, 'rating-item')\n",
    "    # print(test.get_attribute('innerHTML'))\n",
    "    print(driver.current_url)\n",
    "    # return\n",
    "    # elts_rating_content = driver.find_elements(By.CLASS_NAME, 'rating-item-content')\n",
    "    # for elt_rating_content in elts_rating_content:\n",
    "    #     print(elt_rating_content.get_attribute('innerHTML'))\n",
    "        # test = elt_rating_content.find_element(By.CLASS_NAME, 'xXx rating-title').get_attribute('href')\n",
    "        # print(test)\n",
    "\n",
    "    # return\n",
    "    elts_rating = soup_movie.find_all('div', class_='rating-item')\n",
    "    print(\"nb elt_ratings:\", len(elts_rating))\n",
    "    for elt_rating in elts_rating:\n",
    "        elt_content = elt_rating.find('div', class_ = 'rating-item-content')\n",
    "        if 'Spectateurs' in elt_content.find('span').text:\n",
    "            # print(elt_content.prettify())\n",
    "            elt_star_rating = elt_content.find('span', class_ = 'stareval-note')\n",
    "            star_rating = elt_star_rating.text.strip()\n",
    "            elt_nb_notes_reviews = elt_star_rating.find_next_sibling()\n",
    "            print(star_rating, elt_nb_notes_reviews.text.strip())\n",
    "\n",
    "\n",
    "def similar_movies(soup_movies):\n",
    "    pass    \n",
    "\n",
    "def scrap_movie(elt_movie, driver):\n",
    "    ''' scrap all movie informations '''\n",
    "    \n",
    "    # get the movie soup\n",
    "    # print(url_site, elt_movie.h2.a.get('href'))\n",
    "    url_movie = url_site + elt_movie.h2.a.get('href')[1:]\n",
    "    print(url_movie)\n",
    "    r = requests.get(url_movie, auth=('user', 'pass'))\n",
    "    soup_movie = BeautifulSoup(r.content, 'html.parser')\n",
    "    # print(soup_movie.prettify())\n",
    "    # print(type(driver), url_movie)\n",
    "    driver.get(url_movie)\n",
    "    print(driver.current_url)\n",
    "    # elts = driver.find_elements(By.XPATH, '/html/body/div[2]/main/section/div/div[3]/div[2]/div')\n",
    "                                        #    /html/body/div[2]/main/section/div/div[3]/div[2]/div[1]\n",
    "    # print(len(elts))\n",
    "    # test = driver.find_element(By.CLASS_NAME, 'rating-item')\n",
    "    # print(len(test))\n",
    "    # return\n",
    "\n",
    "    # ------------- #\n",
    "    #     Title     #\n",
    "    # ------------- #\n",
    "    title = get_title(soup_movie)\n",
    "    print(\"Title:\" , title)\n",
    "\n",
    "    # --------------------------------- #\n",
    "    #   Date, duration and categories   #\n",
    "    # --------------------------------- #\n",
    "    date, duration, categories = get_date_duration_categories(soup_movie)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Duration:\", duration)\n",
    "    print(\"Categories:\", categories)\n",
    "    \n",
    "    if 'Documentaire' in categories:\n",
    "        print('Documentaire and more categories', categories)\n",
    "        # return (We do not scrap the documentaries ???)\n",
    "\n",
    "    # --------------------------------- #\n",
    "    #   Directors list / Actors list    #\n",
    "    # --------------------------------- #    \n",
    "    lst_directors, lst_actors = [], []\n",
    "    elts_end_section = soup_movie.find_all('a', class_ = 'end-section-link')\n",
    "    \n",
    "    for elt_end_section in elts_end_section:\n",
    "        if 'Casting' in elt_end_section['title']:\n",
    "            # If there is a link to the whole casting\n",
    "            link_casting = elt_end_section['href']\n",
    "            r = requests.get(url_site + link_casting, auth=('user', 'pass'))\n",
    "            soup_casting = BeautifulSoup(r.content, 'html.parser')\n",
    "            # Get directors' list\n",
    "            lst_directors = get_directors(soup_casting)\n",
    "            # Get actors' list\n",
    "            lst_actors = get_actors(soup_casting)\n",
    "\n",
    "        else: # No link to the casting (for example animation movies)\n",
    "            # print(elt_end_section.prettify())\n",
    "            elts = soup_movie.find_all('div', class_ = \"meta-body-item meta-body-direction meta-body-oneline\")\n",
    "            # Get directors' list\n",
    "            lst_directors = [elts[0].text.strip()[2:].strip()]\n",
    "\n",
    "            # For rhis kind of movies we scrap only one director (to see ...)\n",
    "            # if len(elts) > 1:\n",
    "            #     elts_span = elts[1].find_all('span')\n",
    "            #     for elt in elts_span:\n",
    "            #         if 'light' in elt['class']:\n",
    "            #             continue\n",
    "            #         if elt.get_text(strip=True) not in lst_directors:\n",
    "            #             lst_directors.append(elt.get_text(strip=True))\n",
    "\n",
    "    print('Directors:', lst_directors)\n",
    "    print('Actors:', lst_actors)\n",
    "\n",
    "    # ------------ #\n",
    "    #   Summary    #\n",
    "    # ------------ #\n",
    "    print(\"Summary:\", get_summary(soup_movie)[:180])\n",
    "\n",
    "    # ------------ #\n",
    "    #   thumbnail  #\n",
    "    # ------------ #\n",
    "    url_thumbnail = get_thumbnail(soup_movie)\n",
    "    print(\"Thumbnail:\", url_thumbnail)\n",
    "    # save_thumbnail(title, url_thumbnail)\n",
    "\n",
    "    # ------------ #\n",
    "    #    ratings   #\n",
    "    # ------------ #\n",
    "    get_ratings(soup_movie, driver)\n",
    "    return\n",
    "\n",
    "# ---------------------------------- #\n",
    "#                                    #\n",
    "#             Main loop              #\n",
    "#                                    #\n",
    "# ---------------------------------- #\n",
    "        \n",
    "\n",
    "# loop on all years to scrap (through links previously scrapped)\n",
    "# then loop on all pages of the year\n",
    "# then loop on all movies on one page\n",
    "# \n",
    "\n",
    "# delete_thumbnails()\n",
    "\n",
    "# Create Selenium driver\n",
    "driver.close()\n",
    "driver = webdriver.Chrome(options = options)\n",
    "print(driver.current_url)\n",
    "\n",
    "for year, url_year in dict_year_link.items():\n",
    "    print(\"Scraping year:\", year, url_year)\n",
    "    \n",
    "    r = requests.get(url_year, auth=('user', 'pass'))\n",
    "    if r.status_code != 200:\n",
    "        print(\"url_site error\")\n",
    "    \n",
    "    soup_year = BeautifulSoup(r.content, 'html.parser')\n",
    "    nb_pages = number_pages_per_year(soup_year)\n",
    "    # print(\"Nb pages :\", nb_pages)\n",
    "\n",
    "    for i in range(nb_pages): # Need to reduce as some movies are totaly unknown with very few informations about\n",
    "        url_year_page = url_year + f'?page={i+1}'\n",
    "        # print(url_year_page)\n",
    "        r = requests.get(url_year_page, auth=('user', 'pass'))\n",
    "        if r.status_code != 200:\n",
    "            print(\"url_site error\")\n",
    "        \n",
    "        soup_movies = BeautifulSoup(r.content, 'html.parser')\n",
    "        elt_movies = soup_movies.find_all('li', class_='mdl')\n",
    "        for elt_movie in elt_movies[:15]:\n",
    "            print(' ---------------------------------------------------------------- ')\n",
    "            scrap_movie(elt_movie, driver)\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
